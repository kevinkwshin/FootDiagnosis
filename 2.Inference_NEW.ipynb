{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pydicom nibabel albumentations --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "!nvidia-smi\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, natsort\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# axis method\n",
    "# 1. PCA (To do)\n",
    "# 2. Minimum Rotational inertia\n",
    "# 3. Ellipse\n",
    "# !pip install kornia albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import label, regionprops, regionprops_table\n",
    "import math, kornia\n",
    "\n",
    "# # inertia axis\n",
    "# def visualize(**images):\n",
    "#     \"\"\"Plot images in one row.\"\"\"\n",
    "#     angles = list()\n",
    "    \n",
    "#     n = len(images)\n",
    "#     plt.figure(figsize=(32, 16))\n",
    "#     for i, (name, image) in enumerate(images.items()):\n",
    "#         if i==0 and image.shape[0] == 3:\n",
    "#             image_main = image.permute(1,2,0).numpy()#.int()\n",
    "#         if torch.is_tensor(image) and image.shape[0] == 3:\n",
    "#             image = image.permute(1,2,0).numpy()#.int()\n",
    "#         else:\n",
    "#             try:\n",
    "#                 image = image.numpy()\n",
    "#             except:\n",
    "#                 pass\n",
    "            \n",
    "#         plt.subplot(1, n, i + 1)\n",
    "#         plt.xticks([])\n",
    "#         plt.yticks([])\n",
    "#         plt.title(' '.join(name.split('_')).title())\n",
    "#         plt.imshow(image_main,cmap='gray')\n",
    "#         if i>=1:\n",
    "#             prop = inertia_axis(image)\n",
    "#             coord_x = int(prop[0])\n",
    "#             coord_y = int(prop[1])\n",
    "#             angle = prop[2]\n",
    "#             offset= 200\n",
    "#             image[coord_y-5:coord_y+5,coord_x-5:coord_x+5] = 0\n",
    "#             image = cv2.line(image,(coord_x-offset, coord_y-int(offset*np.tan(angle))),(coord_x+offset, coord_y+int(offset*np.tan(angle))),(0,255,255),5)\n",
    "#             angle = np.degrees(angle)*(-1)\n",
    "#             angles.append(angle)\n",
    "#             plt.title(' '.join(name.split('_')).title()+'_'+'{:.2f}'.format(angle))\n",
    "#             plt.imshow(image,cmap='gray',alpha=0.4)\n",
    "#             plt.plot(coord_x, coord_y, '.g', markersize=15)\n",
    "#     plt.show()\n",
    "#     return angles\n",
    "\n",
    "# # ellipse\n",
    "# def visualize(**images):\n",
    "#     \"\"\"Plot images in one row.\"\"\"\n",
    "#     angles = list()\n",
    "    \n",
    "#     n = len(images)\n",
    "#     plt.figure(figsize=(32, 16))\n",
    "#     for i, (name, image) in enumerate(images.items()):\n",
    "#         if i==0 and image.shape[0] == 3:\n",
    "#             image_main = image.permute(1,2,0).numpy()#.int()\n",
    "#         if torch.is_tensor(image) and image.shape[0] == 3:\n",
    "#             image = image.permute(1,2,0).numpy()#.int()\n",
    "#         else:\n",
    "#             try:\n",
    "#                 image = image.numpy()\n",
    "#             except:\n",
    "#                 pass\n",
    "            \n",
    "#         plt.subplot(1, n, i + 1)\n",
    "#         plt.xticks([])\n",
    "#         plt.yticks([])\n",
    "#         plt.title(' '.join(name.split('_')).title())\n",
    "#         plt.imshow(image_main,cmap='gray')\n",
    "#         if i>=1:            \n",
    "#             image = image.astype(np.uint8)\n",
    "#             regions = regionprops(image)\n",
    "#             for props in regions:\n",
    "#                 y0, x0 = props.centroid\n",
    "#                 orientation = props.orientation\n",
    "#                 x1 = x0 + math.sin(orientation) * 0.5 * props.major_axis_length\n",
    "#                 y1 = y0 + math.cos(orientation) * 0.5 * props.major_axis_length\n",
    "#                 x2 = x0 - math.sin(orientation) * 0.5 * props.major_axis_length\n",
    "#                 y2 = y0 - math.cos(orientation) * 0.5 * props.major_axis_length\n",
    "#                 x3 = x0 + math.cos(orientation) * 0.5 * props.minor_axis_length\n",
    "#                 y3 = y0 - math.sin(orientation) * 0.5 * props.minor_axis_length\n",
    "#                 x4 = x0 - math.cos(orientation) * 0.5 * props.minor_axis_length\n",
    "#                 y4 = y0 + math.sin(orientation) * 0.5 * props.minor_axis_length\n",
    "#                 if i<5:\n",
    "#                     angle = - np.tan((y0 - y2) / (x0 - x2))\n",
    "#                     plt.plot((x0, x1), (y0, y1), 'black', linewidth=1)\n",
    "#                     plt.plot((x0, x2), (y0, y2), 'black', linewidth=1)\n",
    "#                     plt.plot(x0, y0, '.g', markersize=15)\n",
    "#                 else:\n",
    "#                     angle = - np.tan((y0 - y3) / (x0 - x3))\n",
    "#                     plt.plot((x0, x3), (y0, y3), 'black', linewidth=1)\n",
    "#                     plt.plot((x0, x4), (y0, y4), 'black', linewidth=1)\n",
    "#                     plt.plot(x0, y0, '.g', markersize=15)\n",
    "                    \n",
    "#                 if angle >= np.pi:\n",
    "#                     angle = angle - np.pi\n",
    "#                 elif angle <= - np.pi:\n",
    "#                     angle = np.pi + angle \n",
    "#                 angle = np.degrees(angle)\n",
    "# #             image[coord_y-5:coord_y+5,coord_x-5:coord_x+5] = 0\n",
    "# #             image = cv2.line(image,(coord_x-offset, coord_y-int(offset*np.tan(angle))),(coord_x+offset, coord_y+int(offset*np.tan(angle))),(0,255,255),5)\n",
    "# #             angle = np.degrees(angle)*(-1)\n",
    "#                 angles.append(angle)\n",
    "# #             angle=0\n",
    "#             try:\n",
    "#                 plt.title(' '.join(name.split('_')).title()+'_'+'{:.2f}'.format(angle))\n",
    "#             except:\n",
    "#                 plt.title(' '.join(name.split('_')).title()+'_')                \n",
    "#             plt.imshow(image,cmap='gray',alpha=0.4)\n",
    "#     plt.show()\n",
    "#     return angles\n",
    "\n",
    "#################### upper is old ##################################### 0\n",
    "\n",
    "# ellipse\n",
    "def visualize(**images):\n",
    "    \"\"\"Plot images in one row.\"\"\"\n",
    "    angles = list()\n",
    "    \n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(32, 16))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        if i==0 and image.shape[0] == 3:\n",
    "            image_main = image.permute(1,2,0).numpy()#.int()\n",
    "        if torch.is_tensor(image) and image.shape[0] == 3:\n",
    "            image = image.permute(1,2,0).numpy()#.int()\n",
    "        else:\n",
    "            try:\n",
    "                image = image.numpy()\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        # plt.title()\n",
    "        plt.imshow(image_main,cmap='gray')        \n",
    "        plt.imshow(image,cmap='gray',alpha=0.4)\n",
    "        if i == 1:\n",
    "            head = head_direction(image)\n",
    "        if i>=1:           \n",
    "            major = True if i <=4 else False\n",
    "            clockwise_plus = True if i==1 else False\n",
    "#             x0,y0,angle = PCA_axis(image,major)\n",
    "            x0,y0,angle = inertia_axis(image,major)\n",
    "            plt.title(' '.join(name.split('_')).title()+'_MMI : {:.2f} degrees'.format(refine_degree(angle,head,clockwise_plus=clockwise_plus)))\n",
    "            image = kornia.morphology.opening(torch.tensor(image).unsqueeze(0).unsqueeze(0),torch.ones(5,5))\n",
    "            image = image.squeeze()\n",
    "            draw_axis(image_main, image, x0, y0, angle)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def draw_axis(image, mask, x0, y0, radian_degree):\n",
    "    \"\"\"\n",
    "    radian_degree\n",
    "    \"\"\"\n",
    "    length = 320\n",
    "    \n",
    "    x1 = x0 + math.cos(radian_degree) * 0.5 * length\n",
    "    y1 = y0 + math.sin(radian_degree) * 0.5 * length\n",
    "    x2 = x0 - math.cos(radian_degree) * 0.5 * length\n",
    "    y2 = y0 - math.sin(radian_degree) * 0.5 * length\n",
    "    x3 = x0 + math.sin(radian_degree) * 0.5 * length\n",
    "    y3 = y0 - math.cos(radian_degree) * 0.5 * length\n",
    "    x4 = x0 - math.sin(radian_degree) * 0.5 * length\n",
    "    y4 = y0 + math.cos(radian_degree) * 0.5 * length\n",
    "\n",
    "    plt.imshow(image,cmap='gray')\n",
    "    plt.imshow(mask,cmap='gray',alpha=0.4)\n",
    "    plt.contour(mask,colors='#FD8A02',alpha=.5)\n",
    "    angle = - np.tan((y0 - y2) / (x0 - x2))\n",
    "    plt.plot((x0, x1), (y0, y1), '#01686D', linewidth=3)\n",
    "    plt.plot((x0, x2), (y0, y2), '#01686D', linewidth=3)\n",
    "    plt.plot(x0, y0, '.g', markersize=15)\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "def raw_moment(data, iord, jord):\n",
    "    nrows, ncols = data.shape\n",
    "    y, x = np.mgrid[:nrows, :ncols]\n",
    "    data = data * x**iord * y**jord\n",
    "    return data.sum() \n",
    "\n",
    "def inertia_axis(data, major):\n",
    "    data_sum = data.sum()\n",
    "    m10 = raw_moment(data, 1, 0)\n",
    "    m01 = raw_moment(data, 0, 1)\n",
    "    x_bar = m10 / data_sum\n",
    "    y_bar = m01 / data_sum\n",
    "    u11 = (raw_moment(data, 1, 1) - x_bar * m01) / data_sum\n",
    "    u20 = (raw_moment(data, 2, 0) - x_bar * m10) / data_sum\n",
    "    u02 = (raw_moment(data, 0, 2) - y_bar * m01) / data_sum\n",
    "    \n",
    "    if major == True:\n",
    "        angle = 0.5 * np.arctan(2 * u11 / (u20 - u02))\n",
    "    else:\n",
    "        angle = 0.5 * np.arctan(2 * u11 / (u20 - u02)) # + np.pi/2\n",
    "    \n",
    "    return x_bar, y_bar, angle\n",
    "\n",
    "def ellipse_axis(image, major=True):\n",
    "    try:\n",
    "        image = image.numpy()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    image = image.astype(np.uint8)\n",
    "    regions = regionprops(image)\n",
    "    for props in regions:\n",
    "        y0, x0 = props.centroid\n",
    "        orientation = props.orientation\n",
    "        x1 = x0 + math.sin(orientation) * 0.5 * 300\n",
    "        y1 = y0 + math.cos(orientation) * 0.5 * 300\n",
    "        x2 = x0 - math.sin(orientation) * 0.5 * 300\n",
    "        y2 = y0 - math.cos(orientation) * 0.5 * 300\n",
    "        x3 = x0 + math.cos(orientation) * 0.5 * 150\n",
    "        y3 = y0 - math.sin(orientation) * 0.5 * 150\n",
    "        x4 = x0 - math.cos(orientation) * 0.5 * 150\n",
    "        y4 = y0 + math.sin(orientation) * 0.5 * 150\n",
    "        \n",
    "        if major:\n",
    "            angle = np.tan((y0 - y2) / (x0 - x2))\n",
    "        else:\n",
    "            angle = np.tan((y0 - y3) / (x0 - x3))\n",
    "\n",
    "    return x0, y0, angle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def PCA_axis(mask_2d,major=True):\n",
    "    \n",
    "    mask_2d = np.rot90(mask_2d,2)\n",
    "    mask_2d = np.flip(mask_2d)\n",
    "    b= np.nonzero(mask_2d == 1)\n",
    "    b = np.array(b)\n",
    "    b = np.transpose(b) \n",
    "    b = np.flip(b) \n",
    "    X = b\n",
    "    \n",
    "#     # visualize\n",
    "#     plt.axis('equal')\n",
    "#     plt.scatter(X[:,0],X[:,1]);    \n",
    "\n",
    "    pca=PCA(2)\n",
    "    pca.fit(X)\n",
    "\n",
    "#     print(\"Principal axes:\", pca.components_)\n",
    "#     print(\"Explained variance:\", pca.explained_variance_)\n",
    "#     print(\"Principal axes angle:\",angle)\n",
    "#     print(\"Mean:\", pca.mean_)\n",
    "\n",
    "    if major == True:\n",
    "        angle = np.arctan(pca.components_[0][1]/pca.components_[0][0])\n",
    "    else:\n",
    "        angle = np.arctan(pca.components_[0][1]/pca.components_[0][0]) - np.pi/2\n",
    "        \n",
    "    x0,y0 = pca.mean_\n",
    "    return x0, y0, angle\n",
    "\n",
    "def refine_degree(radian, head, clockwise_plus=True):\n",
    "    while radian <= -np.pi/2:\n",
    "        radian += np.pi\n",
    "    while radian >= np.pi/2:\n",
    "        radian -= np.pi\n",
    "    \n",
    "    degree = np.degrees(radian)\n",
    "\n",
    "    if head=='left' and clockwise_plus==True:\n",
    "        return degree\n",
    "    elif head=='right' and clockwise_plus==True:\n",
    "        return -degree\n",
    "    elif head=='left' and clockwise_plus==False:\n",
    "        return -degree\n",
    "    elif head=='right' and clockwise_plus==False:\n",
    "        return degree\n",
    "    \n",
    "def head_direction(mask_1_2d):\n",
    "    y,x = mask_1_2d.shape  \n",
    "    x0,y0,_ = inertia_axis(mask_1_2d,True)\n",
    "    ratio = x0/x\n",
    "    if ratio > 0.5: \n",
    "        head = 'left'\n",
    "    else:\n",
    "        head = 'right'\n",
    "    return head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 4\n",
    "# major = True if idx <4 else False\n",
    "# head = head_direction((y_seg[0]))\n",
    "# clockwise_plus = True if idx==0 else False\n",
    "\n",
    "# # x0,y0,angle = PCA_axis(x,major)\n",
    "# # plt.title('MMI : {:.2f} degrees'.format(refine_degree(angle,head,clockwise_plus=clockwise_plus)))\n",
    "# # print(major,head)\n",
    "\n",
    "# plt.figure(figsize=(20,20))\n",
    "# plt.subplot(131)\n",
    "# x0,y0,angle = PCA_axis(y_seg[idx],major)\n",
    "# print(refine_degree(angle,head,clockwise_plus=clockwise_plus))\n",
    "# # plt.title('PCA_{:.3f} degrees'.format(refine_degree(angle)))\n",
    "# plt.axis('off')\n",
    "# draw_axis(x[0], y_seg[idx].numpy(), x0, y0, angle)\n",
    "\n",
    "# plt.subplot(132)\n",
    "# x0,y0,angle = inertia_axis(y_seg[idx],True) \n",
    "# print(refine_degree(angle,head,clockwise_plus=clockwise_plus))\n",
    "# plt.axis('off')\n",
    "# draw_axis(x[0], y_seg[idx].numpy(), x0, y0, angle)\n",
    "\n",
    "# plt.subplot(133)\n",
    "# x0,y0,angle = ellipse_axis(y_seg[idx],major)\n",
    "# print(refine_degree(angle,head,clockwise_plus=clockwise_plus))\n",
    "# plt.axis('off')\n",
    "# draw_axis(x[0], y_seg[idx].numpy(), x0, y0, angle)\n",
    "\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "\n",
    "Writing helper class for data extraction, tranformation and preprocessing  \n",
    "https://pytorch.org/docs/stable/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Augmentation\n",
    "# import albumentations as albu\n",
    "# def get_training_augmentation():\n",
    "#     train_transform = [\n",
    "        \n",
    "#         albu.HorizontalFlip(p=0.5),\n",
    "#         albu.ShiftScaleRotate(scale_limit=0.05, rotate_limit=10, shift_limit=0.02, border_mode=4, p=.5),\n",
    "#         albu.IAAAdditiveGaussianNoise(scale=(1, 5),p=0.2),\n",
    "#         albu.IAAPerspective(scale=(0.01, 0.02),p=0.5),\n",
    "        \n",
    "#         albu.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=False, p=0.5),\n",
    "#         albu.RandomGamma(gamma_limit=(80,120), p=.5),\n",
    "#         albu.RandomToneCurve(scale=0.1,p=.5), \n",
    "#         albu.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=.5),\n",
    "\n",
    "#         albu.OneOf(\n",
    "#             [\n",
    "#                 albu.IAASharpen(p=.5),        \n",
    "#                 albu.GaussNoise(var_limit=0.02, mean=0, p=0.5),\n",
    "#                 albu.MultiplicativeNoise(multiplier=(0.98, 1.02), p=0.5),\n",
    "#                 albu.ISONoise(color_shift=(0.01, 0.02),intensity=(0.1, 0.3),p=0.5),\n",
    "#             ],\n",
    "#             p=0.5,\n",
    "#         ),\n",
    "        \n",
    "#         albu.PadIfNeeded(2500, 2500, border_mode=cv2.BORDER_CONSTANT, value=0, always_apply=True),\n",
    "#         albu.CenterCrop(1024, 2048, always_apply=True),\n",
    "# #         albu.PadIfNeeded(2560, 2560, border_mode=cv2.BORDER_CONSTANT, value=0, always_apply=True),\n",
    "# #         albu.CenterCrop(1280, 2560, always_apply=True),\n",
    "#         albu.Resize(512, 1024, interpolation=cv2.INTER_CUBIC, always_apply=True),\n",
    "#         albu.CLAHE(p=1,always_apply=True),\n",
    "#     ]\n",
    "#     return albu.Compose(train_transform)\n",
    "\n",
    "\n",
    "# def get_validation_augmentation():\n",
    "#     \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "#     test_transform = [\n",
    "#         # albu.CLAHE(clip_limit=(2,2),p=1,always_apply=True),\n",
    "#         # albu.PadIfNeeded(2500, 2500, border_mode=cv2.BORDER_CONSTANT, value=0, always_apply=True),\n",
    "#         # albu.CenterCrop(1024, 2048, always_apply=True),\n",
    "#         # albu.PadIfNeeded(2560, 2560, border_mode=cv2.BORDER_CONSTANT, value=0, always_apply=True),\n",
    "# #         albu.CenterCrop(1280, 2560, always_apply=True),\n",
    "#         # albu.Resize(512, 1024, interpolation=cv2.INTER_CUBIC, always_apply=True),\n",
    "#     ]\n",
    "#     return albu.Compose(test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & Validation Transform chain\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import *\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "from monai.data import (\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    "    CacheDataset,\n",
    "    decollate_batch,\n",
    ")\n",
    "\n",
    "def check(x):\n",
    "    print(x.shape)\n",
    "    return x\n",
    "\n",
    "prob=0.3\n",
    "patch_size= (512,1024)\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"seg\"]),\n",
    "        # AddChanneld(keys=[\"image\"]),\n",
    "        Lambdad(keys='image', func=lambda x: np.transpose(x)),       \n",
    "        Lambdad(keys='image', func=lambda x: cv2.cvtColor(x,cv2.COLOR_GRAY2RGB)),\n",
    "        Lambdad(keys='image', func=lambda x: np.moveaxis(x,-1,0)),\n",
    "        Lambdad(keys='seg', func=lambda x: np.moveaxis(x,-1,0)),\n",
    "        \n",
    "        # EnsureChannelFirstd(keys=[\"image\", \"seg\"]),\n",
    "        ScaleIntensityRanged(keys=[\"image\"],a_min=0, a_max=255, b_min=0.0, b_max=1.0, clip=True,),\n",
    "        \n",
    "        # Spacingd(\n",
    "        #     keys=[\"image\", \"seg\"],\n",
    "        #     pixdim=(1.5, 1.5, 2.0),\n",
    "        #     mode=(\"bilinear\", \"nearest\"),\n",
    "        # ),\n",
    "        \n",
    "        RandAdjustContrastd(keys=[\"image\"],gamma=(0.5, 4.5), prob=prob),\n",
    "        RandShiftIntensityd(keys=[\"image\"],offsets=(-0.3, 0.3), prob=prob),\n",
    "        RandScaleIntensityd(keys=[\"image\"],factors=(-0.3, 0.3), prob=prob),\n",
    "        RandStdShiftIntensityd(keys=['image'],factors=(-5, 5), prob=prob),   \n",
    "        RandHistogramShiftd(keys=[\"image\"], num_control_points=20, prob=prob),\n",
    "        # RandLambdad(keys='image', func=lambda x: 1-x, prob=prob),\n",
    "        # ]),\n",
    "        \n",
    "        # OneOf(transforms=[\n",
    "        RandGaussianSmoothd(keys=[\"image\"],sigma_x=(0.25, 1.5), sigma_y=(0.25, 1.5), sigma_z=(0.25, 1.5), prob=prob),\n",
    "        RandGaussianSharpend(keys=[\"image\"],sigma1_x=(0.5, 1.0), sigma1_y=(0.5, 1.0), sigma1_z=(0.5, 1.0), sigma2_x=0.5, sigma2_y=0.5, sigma2_z=0.5, alpha=(10.0, 30.0), prob=prob),\n",
    "        # ]),\n",
    "            \n",
    "        # OneOf(transforms=[\n",
    "        RandGaussianNoised(keys=[\"image\"],mean=0.0, std=0.1,prob=prob),\n",
    "        RandBiasFieldd(keys=[\"image\"], degree=3, coeff_range=(0.0, 0.1), prob=prob),\n",
    "        RandGibbsNoised(keys=[\"image\"], alpha=(0.0, 1.0), prob=prob),\n",
    "        # RandKSpaceSpikeNoised(keys=[\"image\"], global_prob=1.0, prob=prob),\n",
    "        # ]),        \n",
    "        \n",
    "        RandZoomd(keys=[\"image\",\"seg\"], min_zoom=0.7, max_zoom=1.2, padding_mode=[\"constant\",\"constant\"], prob=prob),\n",
    "        RandRotated(keys=[\"image\",\"seg\"], range_x=[-.2,.2], range_y=[-.2,.2], range_z=[-.2,.2], prob=prob),\n",
    "        RandAffined(keys=[\"image\",\"seg\"], rotate_range=None, shear_range=[-.2,.2], translate_range=[-.15,.15], padding_mode=['zeros','zeros'], prob=1),\n",
    "        Rand2DElasticd(keys=[\"image\",\"seg\"], spacing=(100,100), magnitude_range=(0.1,3), prob=prob),\n",
    "        # CropForegroundd(keys=[\"image\", \"seg\"], source_key=\"image\"),\n",
    "        \n",
    "        RandFlipd(\n",
    "            keys=[\"image\", \"seg\"],\n",
    "            spatial_axis=[0,1], # [1]\n",
    "            prob=prob,\n",
    "        ),\n",
    "        \n",
    "        Resized(keys=[\"image\",\"seg\"],spatial_size=1024, size_mode='longest',mode=['bicubic','nearest']),\n",
    "        Lambdad(keys=['image'], func=lambda x: clahe(x,0)),\n",
    "        RandLambdad(keys=['image'], func=lambda x: 1-x, prob=prob),\n",
    "        SpatialPadd(keys=[\"image\",\"seg\"], method='symmetric', spatial_size=(512, 1024)),\n",
    "        # ResizeWithPadOrCropd(keys=[\"image\",\"seg\"],spatial_size=[768, 1536],mode=['constant','constant']),\n",
    "        # CropForegroundd(keys=[\"image\",\"seg\"],source_key=\"seg\", margin = 350,),\n",
    "        # Resized(keys=[\"image\",\"seg\"],spatial_size=1024, size_mode='longest',mode=['bicubic','nearest']),\n",
    "        # RandCropByPosNegLabeld(\n",
    "        #     keys=[\"image\", \"seg\"],\n",
    "        #     label_key=\"seg\",\n",
    "        #     spatial_size=(512, 1024),\n",
    "        #     pos=1,\n",
    "        #     neg=.5,\n",
    "        #     num_samples=1,\n",
    "        #     image_key=\"image\",\n",
    "        #     image_threshold=0,\n",
    "        # ),\n",
    "        # Resized(keys=[\"image\",\"seg\"],spatial_size=1024, size_mode='longest'),\n",
    "        ToTensord(keys=[\"image\", \"seg\"]),\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"seg\"]),\n",
    "        # EnsureChannelFirst\n",
    "        # AddChanneld(keys=[\"image\"]),\n",
    "        # Lambdad(keys='image', func=lambda x: check(x)),\n",
    "        Lambdad(keys='image', func=lambda x: np.transpose(x)),\n",
    "        Lambdad(keys='image', func=lambda x: cv2.cvtColor(x,cv2.COLOR_GRAY2RGB)),\n",
    "        Lambdad(keys='image', func=lambda x: np.moveaxis(x,-1,0)),\n",
    "        Lambdad(keys='seg', func=lambda x: np.moveaxis(x,-1,0)),\n",
    "        ScaleIntensityRanged(keys=[\"image\"],a_min=0, a_max=255, b_min=0.0, b_max=1.0, clip=True,),\n",
    "        Resized(keys=[\"image\",\"seg\"],spatial_size=1024, size_mode='longest',mode=['bicubic','nearest']),\n",
    "        Lambdad(keys='image', func=lambda x: clahe(x,0)),\n",
    "        # CropForegroundd(keys=[\"image\",\"seg\"], source_key=\"seg\", margin = 300),\n",
    "        CropForegroundd(keys=[\"image\",\"seg\"], source_key=\"seg\", margin = 200),\n",
    "        SpatialPadd(keys=[\"image\",\"seg\"], method='symmetric', spatial_size=(512, 1024)),\n",
    "        CenterSpatialCropd(keys=[\"image\",\"seg\"], roi_size=(512, 1024)), # Others\n",
    "        # SpatialCropd(keys=[\"image\",\"seg\"], roi_start=(0, 0), roi_end=(512, 1024)), # MMA\n",
    "        ToTensord(keys=[\"image\", \"seg\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "import mclahe\n",
    "\n",
    "def clahe(img, dim = -1, adaptive_hist_range=False):\n",
    "    \"\"\"\n",
    "    input 1 numpy shape image (H x W x (D) x C)\n",
    "    \"\"\"\n",
    "    temp = np.zeros_like(img)\n",
    "    if dim == -1:\n",
    "        for idx in range(temp.shape[-1]):\n",
    "            temp[...,idx] = mclahe.mclahe(img[...,idx], kernel_size=None, n_bins=128, clip_limit=0.04, adaptive_hist_range=adaptive_hist_range)\n",
    "    elif dim == 0:\n",
    "        for idx in range(temp.shape[0]):\n",
    "            temp[idx] = mclahe.mclahe(img[idx], kernel_size=None, n_bins=128, clip_limit=0.04, adaptive_hist_range=adaptive_hist_range)\n",
    "    \n",
    "    return temp\n",
    "\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "# DATA_DIR = 'ActiveLearning/1st'\n",
    "# DATA_DIR = 'ActiveLearning/2nd'\n",
    "# DATA_DIR = 'ActiveLearning/3rd'\n",
    "DATA_DIR = 'ActiveLearning/4th'\n",
    "\n",
    "# DATA_DIR = 'NonActiveLearning/1st'\n",
    "# DATA_DIR = 'NonActiveLearning/2nd'\n",
    "# DATA_DIR = 'NonActiveLearning/3rd'\n",
    "# DATA_DIR = 'NonActiveLearning/4th'\n",
    "\n",
    "x_train = natsort.natsorted(glob.glob(os.path.join(DATA_DIR, 'x_train','*.png')))\n",
    "y_train = natsort.natsorted(glob.glob(os.path.join(DATA_DIR, 'y_train','*.npy')))\n",
    "\n",
    "# sklearn의 train_test_split를 사용해 데이터를 나눕니다. 반복성 있게 데이터가 나누어 집니다.\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train,y_train,test_size=0.1,random_state=42)\n",
    "\n",
    "x_test = natsort.natsorted(glob.glob(os.path.join(DATA_DIR, 'x_test','*.png')))\n",
    "y_test = natsort.natsorted(glob.glob(os.path.join(DATA_DIR, 'y_test','*.npy')))\n",
    "train_files = [{\"image\": img, \"seg\": seg, \"fname\": img} for img, seg in zip(x_train, y_train)]\n",
    "val_files = [{\"image\": img, \"seg\": seg, \"fname\": img} for img, seg in zip(x_valid, y_valid)]\n",
    "test_files = [{\"image\": img, \"seg\": seg, \"fname\": img} for img, seg in zip(x_test, y_test)]\n",
    "\n",
    "DATA_DIR = 'ExternalObserver/GP1/'\n",
    "x_testGP1 = natsort.natsorted(glob.glob(os.path.join(DATA_DIR, 'x_test','*.png')))\n",
    "y_testGP1 = natsort.natsorted(glob.glob(os.path.join(DATA_DIR, 'y_test','*.npy')))\n",
    "\n",
    "DATA_DIR = 'ExternalObserver/GP2/'\n",
    "x_testGP2 = natsort.natsorted(glob.glob(os.path.join(DATA_DIR, 'x_test','*.png')))\n",
    "y_testGP2 = natsort.natsorted(glob.glob(os.path.join(DATA_DIR, 'y_test','*.npy')))\n",
    "\n",
    "DATA_DIR = 'ExternalObserver/OS/'\n",
    "x_testOS = natsort.natsorted(glob.glob(os.path.join(DATA_DIR, 'x_test','*.png')))\n",
    "y_testOS = natsort.natsorted(glob.glob(os.path.join(DATA_DIR, 'y_test','*.npy')))\n",
    "\n",
    "x_etest = natsort.natsorted(glob.glob(os.path.join('ExternalValidation', 'LERA_WBLR_seg','*.png')))\n",
    "y_etest = natsort.natsorted(glob.glob(os.path.join('ExternalValidation', 'LERA_WBLR_seg','*.npy')))\n",
    "\n",
    "x_etest2 = natsort.natsorted(glob.glob(os.path.join('ExternalValidation', 'AMC_100_WBLR_seg','refined','*.png')))\n",
    "y_etest2 = natsort.natsorted(glob.glob(os.path.join('ExternalValidation', 'AMC_100_WBLR_seg','refined','*.npy')))\n",
    "\n",
    "train_files = [{\"image\": img, \"seg\": seg, \"fname\": img} for img, seg in zip(x_train, y_train)]\n",
    "val_files = [{\"image\": img, \"seg\": seg, \"fname\": img} for img, seg in zip(x_valid, y_valid)]\n",
    "test_files = [{\"image\": img, \"seg\": seg, \"fname\": img} for img, seg in zip(x_test, y_test)]\n",
    "testGP1_files = [{\"image\": img, \"seg\": seg, \"fname\": img} for img, seg in zip(x_testGP1, y_testGP1)]\n",
    "testGP2_files = [{\"image\": img, \"seg\": seg, \"fname\": img} for img, seg in zip(x_testGP2, y_testGP2)]\n",
    "testOS_files = [{\"image\": img, \"seg\": seg, \"fname\": img} for img, seg in zip(x_testOS, y_testOS)]\n",
    "etest_files = [{\"image\": img, \"seg\": seg, \"fname\": img} for img, seg in zip(x_etest, y_etest)]\n",
    "etest2_files = [{\"image\": img, \"seg\": seg, \"fname\": img} for img, seg in zip(x_etest2, y_etest2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = CacheDataset(\n",
    "#     data=train_files,\n",
    "#     transform=train_transforms,\n",
    "#     cache_num=8,\n",
    "#     cache_rate=1.0,\n",
    "#     num_workers=4,\n",
    "# )\n",
    "# train_loader = DataLoader(\n",
    "#     train_ds, batch_size=1, shuffle=True, num_workers=4, pin_memory=True\n",
    "# )\n",
    "\n",
    "# val_ds = CacheDataset(\n",
    "#     data=val_files,\n",
    "#     transform=val_transforms,\n",
    "#     cache_num=4,\n",
    "#     cache_rate=1.0,\n",
    "#     num_workers=4\n",
    "# )\n",
    "# val_loader = DataLoader(\n",
    "#     val_ds, batch_size=1, shuffle=False, num_workers=2, pin_memory=True\n",
    "# )\n",
    "\n",
    "test_ds = Dataset(\n",
    "    data=test_files,\n",
    "    transform=val_transforms,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=1, shuffle=False, num_workers=1, pin_memory=True\n",
    ")\n",
    "\n",
    "testGP1_ds = Dataset(\n",
    "    data=testGP1_files,\n",
    "    transform=val_transforms,\n",
    ")\n",
    "\n",
    "testGP1_loader = DataLoader(\n",
    "    testGP1_ds, batch_size=1, shuffle=False, num_workers=1, pin_memory=True\n",
    ")\n",
    "\n",
    "testGP2_ds = Dataset(\n",
    "    data=testGP2_files,\n",
    "    transform=val_transforms,\n",
    ")\n",
    "\n",
    "testGP2_loader = DataLoader(\n",
    "    testGP2_ds, batch_size=1, shuffle=False, num_workers=1, pin_memory=True\n",
    ")\n",
    "\n",
    "testOS_ds = Dataset(\n",
    "    data=testOS_files,\n",
    "    transform=val_transforms,\n",
    ")\n",
    "\n",
    "testOS_loader = DataLoader(\n",
    "    testOS_ds, batch_size=1, shuffle=False, num_workers=1, pin_memory=True\n",
    ")\n",
    "\n",
    "etest_ds = Dataset(\n",
    "    data=etest_files,\n",
    "    transform=val_transforms,\n",
    ")\n",
    "\n",
    "etest_loader = DataLoader(\n",
    "    etest_ds, batch_size=1, shuffle=False, num_workers=1, pin_memory=True\n",
    ")\n",
    "\n",
    "etest2_ds = Dataset(\n",
    "    data=etest2_files,\n",
    "    transform=val_transforms,\n",
    ")\n",
    "\n",
    "etest2_loader = DataLoader(\n",
    "    etest2_ds, batch_size=1, shuffle=False, num_workers=1, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator = iter(train_loader)\n",
    "# iterator = iter(val_loader)\n",
    "# iterator = iter(test_loader)\n",
    "# iterator = iter(etest_loader)\n",
    "batch = iterator.next()\n",
    "x = batch['image']\n",
    "y = batch['seg']\n",
    "print(x.shape, y.shape)\n",
    "for idx in range(len(x)):\n",
    "    plt.imshow(x[idx,0],cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(test_loader)\n",
    "batch = iterator.next()\n",
    "x = batch['image']\n",
    "y = batch['seg']\n",
    "print(x.shape, y.shape)\n",
    "for idx in range(len(x)):\n",
    "    plt.imshow(x[idx,0],cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(testGP1_loader)\n",
    "batch = iterator.next()\n",
    "x = batch['image']\n",
    "y = batch['seg']\n",
    "print(x.shape, y.shape)\n",
    "for idx in range(len(x)):\n",
    "    plt.imshow(x[idx,0],cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(testGP2_loader)\n",
    "batch = iterator.next()\n",
    "x = batch['image']\n",
    "y = batch['seg']\n",
    "print(x.shape, y.shape)\n",
    "for idx in range(len(x)):\n",
    "    plt.imshow(x[idx,0],cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(testOS_loader)\n",
    "batch = iterator.next()\n",
    "x = batch['image']\n",
    "y = batch['seg']\n",
    "print(x.shape, y.shape)\n",
    "for idx in range(len(x)):\n",
    "    plt.imshow(x[idx,0],cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator = iter(train_loader)\n",
    "# iterator = iter(val_loader)\n",
    "# iterator = iter(test_loader)\n",
    "iterator = iter(etest_loader)\n",
    "batch = iterator.next()\n",
    "x = batch['image']\n",
    "y = batch['seg']\n",
    "print(x.shape, y.shape)\n",
    "for idx in range(len(x)):\n",
    "    plt.imshow(x[idx,0],cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(etest2_loader)\n",
    "batch = iterator.next()\n",
    "x = batch['image']\n",
    "y = batch['seg']\n",
    "print(x.shape, y.shape)\n",
    "for idx in range(len(x)):\n",
    "    plt.imshow(x[idx,0],cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "# import sklearn.model_selection \n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# x_train = natsort.natsorted(glob.glob(os.path.join(STAGE, 'x_train','*.png')))\n",
    "# y_train = natsort.natsorted(glob.glob(os.path.join(STAGE, 'y_train','*.npy')))\n",
    "\n",
    "# # x_train, x_valid, y_train, y_valid = train_test_split(x_train,y_train,test_size=0.1, random_state=42)\n",
    "\n",
    "# x_test = natsort.natsorted(glob.glob(os.path.join(STAGE, 'x_test','*.png')))\n",
    "# y_test = natsort.natsorted(glob.glob(os.path.join(STAGE, 'y_test','*.npy')))\n",
    "\n",
    "# x_inference = natsort.natsorted(glob.glob(os.path.join(STAGE, 'x_inference','*')))\n",
    "# y_inference = natsort.natsorted(glob.glob(os.path.join(STAGE, 'y_inference','*')))\n",
    "\n",
    "# x_etest = natsort.natsorted(glob.glob(os.path.join('ExternalValidation','LERA_WBLR_seg','*.png')))\n",
    "# y_etest = natsort.natsorted(glob.glob(os.path.join('ExternalValidation','LERA_WBLR_seg','*.npy')))\n",
    "\n",
    "# train_dataset = Dataset(\n",
    "#     x_train, \n",
    "#     y_train, \n",
    "#     augmentation=get_validation_augmentation(), \n",
    "# )\n",
    "\n",
    "\n",
    "# valid_dataset = Dataset(\n",
    "#     x_valid, \n",
    "#     y_valid, \n",
    "#     augmentation=get_validation_augmentation(), \n",
    "# ) \n",
    "\n",
    "# test_dataset = Dataset(\n",
    "#     x_test, \n",
    "#     y_test, \n",
    "#     augmentation=get_validation_augmentation(), \n",
    "# )\n",
    "\n",
    "# etest_dataset = Dataset(\n",
    "#     x_etest, \n",
    "#     y_etest, \n",
    "#     augmentation=get_validation_augmentation(), \n",
    "# )\n",
    "\n",
    "# inference_dataset = Dataset(\n",
    "#     x_inference, \n",
    "#     y_inference, \n",
    "#     augmentation=get_validation_augmentation(), \n",
    "# )\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "# valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "# etest_loader = DataLoader(etest_dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "# inference_loader = DataLoader(inference_dataset, batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset\n",
    "batch = next(iter(train_loader))\n",
    "x = batch['image'][0]\n",
    "y_seg = batch['seg'][0]\n",
    "fname = batch['fname'][0]\n",
    "print(fname,x.shape,y_seg.shape)\n",
    "visualize(image=x, mask1=y_seg[0], mask2=y_seg[1], mask3=y_seg[2], mask4=y_seg[3], mask5=y_seg[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testset\n",
    "batch = next(iter(test_loader))\n",
    "x = batch['image'][0]\n",
    "y_seg = batch['seg'][0]\n",
    "fname = batch['fname'][0]\n",
    "print(fname)\n",
    "\n",
    "visualize(image=x, mask1=y_seg[0], mask2=y_seg[1], mask3=y_seg[2], mask4=y_seg[3], mask5=y_seg[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testset\n",
    "batch = next(iter(testGP1_loader))\n",
    "x = batch['image'][0]\n",
    "y_seg = batch['seg'][0]\n",
    "fname = batch['fname'][0]\n",
    "print(fname)\n",
    "\n",
    "visualize(image=x, mask1=y_seg[0], mask2=y_seg[1], mask3=y_seg[2], mask4=y_seg[3], mask5=y_seg[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testset\n",
    "\n",
    "batch = next(iter(etest_loader))\n",
    "x = batch['image'][0]\n",
    "y_seg = batch['seg'][0]\n",
    "fname = batch['fname'][0]\n",
    "print(fname, x.shape)\n",
    "\n",
    "visualize(image=x, mask1=y_seg[0], mask2=y_seg[1], mask3=y_seg[2], mask4=y_seg[3], mask5=y_seg[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testset\n",
    "\n",
    "# batch = next(iter(inference_loader))\n",
    "# x = batch['x'][0]\n",
    "# y_seg = batch['y_seg'][0]\n",
    "# fname = batch['fname'][0]\n",
    "# print(fname)\n",
    "\n",
    "# visualize(image=x, mask1=y_seg[0], mask2=y_seg[1], mask3=y_seg[2], mask4=y_seg[3], mask5=y_seg[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all\n",
    "angles_total = list()\n",
    "for idx, batch in enumerate(train_loader):\n",
    "    x = batch['x'][0]\n",
    "    y_seg = batch['y_seg'][0]\n",
    "    fname = batch['fname'][0]\n",
    "    print(idx, fname)\n",
    "    angles = visualize(image=x, mask1=y_seg[0], mask2=y_seg[1], mask3=y_seg[2], mask4=y_seg[3], mask5=y_seg[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all\n",
    "angles_total = list()\n",
    "for idx, batch in enumerate(test_loader):\n",
    "    x = batch['image'][0]\n",
    "    y_seg = batch['seg'][0]\n",
    "    fname = batch['fname'][0]\n",
    "    print(idx, fname)\n",
    "    visualize(image=x, mask1=y_seg[0], mask2=y_seg[1], mask3=y_seg[2], mask4=y_seg[3], mask5=y_seg[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all\n",
    "angles_total = list()\n",
    "for idx, batch in enumerate(etest_loader):\n",
    "    x = batch['image'][0]\n",
    "    y_seg = batch['seg'][0]\n",
    "    fname = batch['fname'][0]\n",
    "    print(idx, fname)\n",
    "    visualize(image=x, mask1=y_seg[0], mask2=y_seg[1], mask3=y_seg[2], mask4=y_seg[3], mask5=y_seg[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles_1 = list()\n",
    "angles_2 = list()\n",
    "angles_3 = list()\n",
    "angles_4 = list()\n",
    "angles_5 = list()\n",
    "\n",
    "for idx in range(len(angles_total)):\n",
    "    angles_1.append(angles_total[idx][0])\n",
    "    angles_2.append(angles_total[idx][1])\n",
    "    angles_3.append(angles_total[idx][2])\n",
    "    angles_4.append(angles_total[idx][3])\n",
    "    angles_5.append(angles_total[idx][4])\n",
    "df = pd.DataFrame([files,angles_1,angles_2,angles_3,angles_4,angles_5],['fname','angles_1','angles_2','angles_3','angles_4','angles_5'])\n",
    "df = df.T\n",
    "# df.to_csv('angles_train_eclipse.csv',index=False)\n",
    "df.to_csv('angles_train_inertia_axis.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all\n",
    "for idx, batch in enumerate(valid_loader):\n",
    "    x = batch['x'][0]\n",
    "    y_seg = batch['y_seg'][0]\n",
    "    fname = batch['fname'][0]\n",
    "    print(fname)\n",
    "\n",
    "    visualize(image=x, mask1=y_seg[0], mask2=y_seg[1], mask3=y_seg[2], mask4=y_seg[3], mask5=y_seg[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all\n",
    "for idx, batch in enumerate(test_loader):\n",
    "    x = batch['x'][0]\n",
    "    y_seg = batch['y_seg'][0]\n",
    "    fname = batch['fname'][0]\n",
    "    print(fname)\n",
    "\n",
    "    visualize(image=x, mask1=y_seg[0], mask2=y_seg[1], mask3=y_seg[2], mask4=y_seg[3], mask5=y_seg[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(1-y_seg[0],cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = batch['x'][0]\n",
    "y_seg = batch['y_seg'][0]\n",
    "fname = batch['fname'][0]\n",
    "#     print(fname)\n",
    "plt.imshow(x[0],cmap='gray')\n",
    "# y = y_seg[0]+y_seg[1]*2+y_seg[2]*3+y_seg[3]*4+y_seg[4]*5\n",
    "y = y_seg[0]#+y_seg[1]+y_seg[2]+y_seg[3]+y_seg[4]\n",
    "plt.imshow(y,cmap='gray',alpha=0.5)\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ellipse_angle(image, major=True):\n",
    "#     image = image.astype(np.uint8)\n",
    "#     regions = regionprops(image)\n",
    "#     for props in regions:\n",
    "#         y0, x0 = props.centroid\n",
    "#         orientation = props.orientation\n",
    "#         x1 = x0 + math.sin(orientation) * 0.5 * props.major_axis_length\n",
    "#         y1 = y0 + math.cos(orientation) * 0.5 * props.major_axis_length\n",
    "#         x2 = x0 - math.sin(orientation) * 0.5 * props.major_axis_length\n",
    "#         y2 = y0 - math.cos(orientation) * 0.5 * props.major_axis_length\n",
    "#         x3 = x0 + math.cos(orientation) * 0.5 * props.minor_axis_length\n",
    "#         y3 = y0 - math.sin(orientation) * 0.5 * props.minor_axis_length\n",
    "#         x4 = x0 - math.cos(orientation) * 0.5 * props.minor_axis_length\n",
    "#         y4 = y0 + math.sin(orientation) * 0.5 * props.minor_axis_length\n",
    "#         if major:\n",
    "#             angle = - np.tan((y0 - y2) / (x0 - x2))\n",
    "#             plt.plot((x0, x1), (y0, y1), 'black', linewidth=1)\n",
    "#             plt.plot((x0, x2), (y0, y2), 'black', linewidth=1)\n",
    "#             plt.plot(x0, y0, '.g', markersize=15)\n",
    "#         else:\n",
    "#             angle = - np.tan((y0 - y3) / (x0 - x3))\n",
    "#             plt.plot((x0, x3), (y0, y3), 'black', linewidth=1)\n",
    "#             plt.plot((x0, x4), (y0, y4), 'black', linewidth=1)\n",
    "#             plt.plot(x0, y0, '.r', markersize=15)\n",
    "\n",
    "#         if angle >= np.pi:\n",
    "#             angle = angle - np.pi\n",
    "#         elif angle <= - np.pi:\n",
    "#             angle = np.pi + angle \n",
    "#         angle = np.degrees(angle)\n",
    "#     return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(x[0],cmap='gray')\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(y_seg[0],cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x[0],cmap='gray')\n",
    "plt.imshow(y_seg[0],cmap='gray',alpha=0.5)\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(5):\n",
    "    major = True if idx<=3 else False\n",
    "    print(ellipse_axis(y_seg[idx].numpy(),major=major))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y,cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "# import tqdm\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODER = 'resnet50'\n",
    "\n",
    "# ENCODER_WEIGHTS = 'imagenet'\n",
    "# ENCODER_WEIGHTS = None\n",
    "# ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "# DEVICE = 'cuda'\n",
    "\n",
    "# # create segmentation model with pretrained encoder\n",
    "\n",
    "# model = smp.Unet(\n",
    "#     encoder_name=ENCODER, \n",
    "#     encoder_weights=ENCODER_WEIGHTS,\n",
    "#     in_channels=3,\n",
    "#     classes=5, \n",
    "#     activation=ACTIVATION,\n",
    "#     decoder_attention_type='scse',\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class REBNCONV(nn.Module):\n",
    "    def __init__(self,in_ch=3,out_ch=3,dirate=1):\n",
    "        super(REBNCONV,self).__init__()\n",
    "\n",
    "        self.conv_s1 = nn.Conv2d(in_ch,out_ch,3,padding=1*dirate,dilation=1*dirate)\n",
    "        self.bn_s1 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu_s1 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hx = x\n",
    "        xout = self.relu_s1(self.bn_s1(self.conv_s1(hx)))\n",
    "\n",
    "        return xout\n",
    "\n",
    "## upsample tensor 'src' to have the same spatial size with tensor 'tar'\n",
    "def _upsample_like(src,tar):\n",
    "\n",
    "    src = F.upsample(src,size=tar.shape[2:],mode='bilinear')\n",
    "\n",
    "    return src\n",
    "\n",
    "\n",
    "### RSU-7 ###\n",
    "class RSU7(nn.Module):#UNet07DRES(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n",
    "        super(RSU7,self).__init__()\n",
    "\n",
    "        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n",
    "        self.pool1 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool2 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool3 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool4 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv5 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool5 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv6 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv7 = REBNCONV(mid_ch,mid_ch,dirate=2)\n",
    "\n",
    "        self.rebnconv6d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv5d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv4d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hx = x\n",
    "        hxin = self.rebnconvin(hx)\n",
    "\n",
    "        hx1 = self.rebnconv1(hxin)\n",
    "        hx = self.pool1(hx1)\n",
    "\n",
    "        hx2 = self.rebnconv2(hx)\n",
    "        hx = self.pool2(hx2)\n",
    "\n",
    "        hx3 = self.rebnconv3(hx)\n",
    "        hx = self.pool3(hx3)\n",
    "\n",
    "        hx4 = self.rebnconv4(hx)\n",
    "        hx = self.pool4(hx4)\n",
    "\n",
    "        hx5 = self.rebnconv5(hx)\n",
    "        hx = self.pool5(hx5)\n",
    "\n",
    "        hx6 = self.rebnconv6(hx)\n",
    "\n",
    "        hx7 = self.rebnconv7(hx6)\n",
    "\n",
    "        hx6d =  self.rebnconv6d(torch.cat((hx7,hx6),1))\n",
    "        hx6dup = _upsample_like(hx6d,hx5)\n",
    "\n",
    "        hx5d =  self.rebnconv5d(torch.cat((hx6dup,hx5),1))\n",
    "        hx5dup = _upsample_like(hx5d,hx4)\n",
    "\n",
    "        hx4d = self.rebnconv4d(torch.cat((hx5dup,hx4),1))\n",
    "        hx4dup = _upsample_like(hx4d,hx3)\n",
    "\n",
    "        hx3d = self.rebnconv3d(torch.cat((hx4dup,hx3),1))\n",
    "        hx3dup = _upsample_like(hx3d,hx2)\n",
    "\n",
    "        hx2d = self.rebnconv2d(torch.cat((hx3dup,hx2),1))\n",
    "        hx2dup = _upsample_like(hx2d,hx1)\n",
    "\n",
    "        hx1d = self.rebnconv1d(torch.cat((hx2dup,hx1),1))\n",
    "\n",
    "        return hx1d + hxin\n",
    "\n",
    "### RSU-6 ###\n",
    "class RSU6(nn.Module):#UNet06DRES(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n",
    "        super(RSU6,self).__init__()\n",
    "\n",
    "        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n",
    "        self.pool1 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool2 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool3 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool4 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv5 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv6 = REBNCONV(mid_ch,mid_ch,dirate=2)\n",
    "\n",
    "        self.rebnconv5d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv4d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hx = x\n",
    "\n",
    "        hxin = self.rebnconvin(hx)\n",
    "\n",
    "        hx1 = self.rebnconv1(hxin)\n",
    "        hx = self.pool1(hx1)\n",
    "\n",
    "        hx2 = self.rebnconv2(hx)\n",
    "        hx = self.pool2(hx2)\n",
    "\n",
    "        hx3 = self.rebnconv3(hx)\n",
    "        hx = self.pool3(hx3)\n",
    "\n",
    "        hx4 = self.rebnconv4(hx)\n",
    "        hx = self.pool4(hx4)\n",
    "\n",
    "        hx5 = self.rebnconv5(hx)\n",
    "\n",
    "        hx6 = self.rebnconv6(hx5)\n",
    "\n",
    "\n",
    "        hx5d =  self.rebnconv5d(torch.cat((hx6,hx5),1))\n",
    "        hx5dup = _upsample_like(hx5d,hx4)\n",
    "\n",
    "        hx4d = self.rebnconv4d(torch.cat((hx5dup,hx4),1))\n",
    "        hx4dup = _upsample_like(hx4d,hx3)\n",
    "\n",
    "        hx3d = self.rebnconv3d(torch.cat((hx4dup,hx3),1))\n",
    "        hx3dup = _upsample_like(hx3d,hx2)\n",
    "\n",
    "        hx2d = self.rebnconv2d(torch.cat((hx3dup,hx2),1))\n",
    "        hx2dup = _upsample_like(hx2d,hx1)\n",
    "\n",
    "        hx1d = self.rebnconv1d(torch.cat((hx2dup,hx1),1))\n",
    "\n",
    "        return hx1d + hxin\n",
    "\n",
    "### RSU-5 ###\n",
    "class RSU5(nn.Module):#UNet05DRES(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n",
    "        super(RSU5,self).__init__()\n",
    "\n",
    "        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n",
    "        self.pool1 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool2 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool3 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv5 = REBNCONV(mid_ch,mid_ch,dirate=2)\n",
    "\n",
    "        self.rebnconv4d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hx = x\n",
    "\n",
    "        hxin = self.rebnconvin(hx)\n",
    "\n",
    "        hx1 = self.rebnconv1(hxin)\n",
    "        hx = self.pool1(hx1)\n",
    "\n",
    "        hx2 = self.rebnconv2(hx)\n",
    "        hx = self.pool2(hx2)\n",
    "\n",
    "        hx3 = self.rebnconv3(hx)\n",
    "        hx = self.pool3(hx3)\n",
    "\n",
    "        hx4 = self.rebnconv4(hx)\n",
    "\n",
    "        hx5 = self.rebnconv5(hx4)\n",
    "\n",
    "        hx4d = self.rebnconv4d(torch.cat((hx5,hx4),1))\n",
    "        hx4dup = _upsample_like(hx4d,hx3)\n",
    "\n",
    "        hx3d = self.rebnconv3d(torch.cat((hx4dup,hx3),1))\n",
    "        hx3dup = _upsample_like(hx3d,hx2)\n",
    "\n",
    "        hx2d = self.rebnconv2d(torch.cat((hx3dup,hx2),1))\n",
    "        hx2dup = _upsample_like(hx2d,hx1)\n",
    "\n",
    "        hx1d = self.rebnconv1d(torch.cat((hx2dup,hx1),1))\n",
    "\n",
    "        return hx1d + hxin\n",
    "\n",
    "### RSU-4 ###\n",
    "class RSU4(nn.Module):#UNet04DRES(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n",
    "        super(RSU4,self).__init__()\n",
    "\n",
    "        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n",
    "        self.pool1 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool2 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=2)\n",
    "\n",
    "        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hx = x\n",
    "\n",
    "        hxin = self.rebnconvin(hx)\n",
    "\n",
    "        hx1 = self.rebnconv1(hxin)\n",
    "        hx = self.pool1(hx1)\n",
    "\n",
    "        hx2 = self.rebnconv2(hx)\n",
    "        hx = self.pool2(hx2)\n",
    "\n",
    "        hx3 = self.rebnconv3(hx)\n",
    "\n",
    "        hx4 = self.rebnconv4(hx3)\n",
    "\n",
    "        hx3d = self.rebnconv3d(torch.cat((hx4,hx3),1))\n",
    "        hx3dup = _upsample_like(hx3d,hx2)\n",
    "\n",
    "        hx2d = self.rebnconv2d(torch.cat((hx3dup,hx2),1))\n",
    "        hx2dup = _upsample_like(hx2d,hx1)\n",
    "\n",
    "        hx1d = self.rebnconv1d(torch.cat((hx2dup,hx1),1))\n",
    "\n",
    "        return hx1d + hxin\n",
    "\n",
    "### RSU-4F ###\n",
    "class RSU4F(nn.Module):#UNet04FRES(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n",
    "        super(RSU4F,self).__init__()\n",
    "\n",
    "        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n",
    "        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=2)\n",
    "        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=4)\n",
    "\n",
    "        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=8)\n",
    "\n",
    "        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=4)\n",
    "        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=2)\n",
    "        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hx = x\n",
    "\n",
    "        hxin = self.rebnconvin(hx)\n",
    "\n",
    "        hx1 = self.rebnconv1(hxin)\n",
    "        hx2 = self.rebnconv2(hx1)\n",
    "        hx3 = self.rebnconv3(hx2)\n",
    "\n",
    "        hx4 = self.rebnconv4(hx3)\n",
    "\n",
    "        hx3d = self.rebnconv3d(torch.cat((hx4,hx3),1))\n",
    "        hx2d = self.rebnconv2d(torch.cat((hx3d,hx2),1))\n",
    "        hx1d = self.rebnconv1d(torch.cat((hx2d,hx1),1))\n",
    "\n",
    "        return hx1d + hxin\n",
    "\n",
    "\n",
    "##### U^2-Net ####\n",
    "class U2NET(nn.Module):\n",
    "\n",
    "    def __init__(self,in_ch=1,out_ch=5):\n",
    "        super(U2NET,self).__init__()\n",
    "\n",
    "        self.stage1 = RSU7(in_ch,32,64)\n",
    "        self.pool12 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.stage2 = RSU6(64,32,128)\n",
    "        self.pool23 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.stage3 = RSU5(128,64,256)\n",
    "        self.pool34 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.stage4 = RSU4(256,128,512)\n",
    "        self.pool45 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.stage5 = RSU4F(512,256,512)\n",
    "        self.pool56 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.stage6 = RSU4F(512,256,512)\n",
    "\n",
    "        # decoder\n",
    "        self.stage5d = RSU4F(1024,256,512)\n",
    "        self.stage4d = RSU4(1024,128,256)\n",
    "        self.stage3d = RSU5(512,64,128)\n",
    "        self.stage2d = RSU6(256,32,64)\n",
    "        self.stage1d = RSU7(128,16,64)\n",
    "\n",
    "        self.side1 = nn.Conv2d(64,out_ch,3,padding=1)\n",
    "        self.side2 = nn.Conv2d(64,out_ch,3,padding=1)\n",
    "        self.side3 = nn.Conv2d(128,out_ch,3,padding=1)\n",
    "        self.side4 = nn.Conv2d(256,out_ch,3,padding=1)\n",
    "        self.side5 = nn.Conv2d(512,out_ch,3,padding=1)\n",
    "        self.side6 = nn.Conv2d(512,out_ch,3,padding=1)\n",
    "\n",
    "        self.outconv = nn.Conv2d(6*out_ch,out_ch,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hx = x\n",
    "\n",
    "        #stage 1\n",
    "        hx1 = self.stage1(hx)\n",
    "        hx = self.pool12(hx1)\n",
    "\n",
    "        #stage 2\n",
    "        hx2 = self.stage2(hx)\n",
    "        hx = self.pool23(hx2)\n",
    "\n",
    "        #stage 3\n",
    "        hx3 = self.stage3(hx)\n",
    "        hx = self.pool34(hx3)\n",
    "\n",
    "        #stage 4\n",
    "        hx4 = self.stage4(hx)\n",
    "        hx = self.pool45(hx4)\n",
    "\n",
    "        #stage 5\n",
    "        hx5 = self.stage5(hx)\n",
    "        hx = self.pool56(hx5)\n",
    "\n",
    "        #stage 6\n",
    "        hx6 = self.stage6(hx)\n",
    "        hx6up = _upsample_like(hx6,hx5)\n",
    "\n",
    "        #-------------------- decoder --------------------\n",
    "        hx5d = self.stage5d(torch.cat((hx6up,hx5),1))\n",
    "        hx5dup = _upsample_like(hx5d,hx4)\n",
    "\n",
    "        hx4d = self.stage4d(torch.cat((hx5dup,hx4),1))\n",
    "        hx4dup = _upsample_like(hx4d,hx3)\n",
    "\n",
    "        hx3d = self.stage3d(torch.cat((hx4dup,hx3),1))\n",
    "        hx3dup = _upsample_like(hx3d,hx2)\n",
    "\n",
    "        hx2d = self.stage2d(torch.cat((hx3dup,hx2),1))\n",
    "        hx2dup = _upsample_like(hx2d,hx1)\n",
    "\n",
    "        hx1d = self.stage1d(torch.cat((hx2dup,hx1),1))\n",
    "\n",
    "\n",
    "        #side output\n",
    "        d1 = self.side1(hx1d)\n",
    "\n",
    "        d2 = self.side2(hx2d)\n",
    "        d2 = _upsample_like(d2,d1)\n",
    "\n",
    "        d3 = self.side3(hx3d)\n",
    "        d3 = _upsample_like(d3,d1)\n",
    "\n",
    "        d4 = self.side4(hx4d)\n",
    "        d4 = _upsample_like(d4,d1)\n",
    "\n",
    "        d5 = self.side5(hx5d)\n",
    "        d5 = _upsample_like(d5,d1)\n",
    "\n",
    "        d6 = self.side6(hx6)\n",
    "        d6 = _upsample_like(d6,d1)\n",
    "\n",
    "        d0 = self.outconv(torch.cat((d1,d2,d3,d4,d5,d6),1))\n",
    "\n",
    "        return F.sigmoid(d0), F.sigmoid(d1), F.sigmoid(d2), F.sigmoid(d3), F.sigmoid(d4), F.sigmoid(d5), F.sigmoid(d6)\n",
    "\n",
    "### U^2-Net small ###\n",
    "class U2NETP(nn.Module):\n",
    "\n",
    "    def __init__(self,in_ch=3,out_ch=1):\n",
    "        super(U2NETP,self).__init__()\n",
    "\n",
    "        self.stage1 = RSU7(in_ch,16,64)\n",
    "        self.pool12 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.stage2 = RSU6(64,16,64)\n",
    "        self.pool23 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.stage3 = RSU5(64,16,64)\n",
    "        self.pool34 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.stage4 = RSU4(64,16,64)\n",
    "        self.pool45 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.stage5 = RSU4F(64,16,64)\n",
    "        self.pool56 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.stage6 = RSU4F(64,16,64)\n",
    "\n",
    "        # decoder\n",
    "        self.stage5d = RSU4F(128,16,64)\n",
    "        self.stage4d = RSU4(128,16,64)\n",
    "        self.stage3d = RSU5(128,16,64)\n",
    "        self.stage2d = RSU6(128,16,64)\n",
    "        self.stage1d = RSU7(128,16,64)\n",
    "\n",
    "        self.side1 = nn.Conv2d(64,out_ch,3,padding=1)\n",
    "        self.side2 = nn.Conv2d(64,out_ch,3,padding=1)\n",
    "        self.side3 = nn.Conv2d(64,out_ch,3,padding=1)\n",
    "        self.side4 = nn.Conv2d(64,out_ch,3,padding=1)\n",
    "        self.side5 = nn.Conv2d(64,out_ch,3,padding=1)\n",
    "        self.side6 = nn.Conv2d(64,out_ch,3,padding=1)\n",
    "\n",
    "        self.outconv = nn.Conv2d(6*out_ch,out_ch,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hx = x\n",
    "\n",
    "        #stage 1\n",
    "        hx1 = self.stage1(hx)\n",
    "        hx = self.pool12(hx1)\n",
    "\n",
    "        #stage 2\n",
    "        hx2 = self.stage2(hx)\n",
    "        hx = self.pool23(hx2)\n",
    "\n",
    "        #stage 3\n",
    "        hx3 = self.stage3(hx)\n",
    "        hx = self.pool34(hx3)\n",
    "\n",
    "        #stage 4\n",
    "        hx4 = self.stage4(hx)\n",
    "        hx = self.pool45(hx4)\n",
    "\n",
    "        #stage 5\n",
    "        hx5 = self.stage5(hx)\n",
    "        hx = self.pool56(hx5)\n",
    "\n",
    "        #stage 6\n",
    "        hx6 = self.stage6(hx)\n",
    "        hx6up = _upsample_like(hx6,hx5)\n",
    "\n",
    "        #decoder\n",
    "        hx5d = self.stage5d(torch.cat((hx6up,hx5),1))\n",
    "        hx5dup = _upsample_like(hx5d,hx4)\n",
    "\n",
    "        hx4d = self.stage4d(torch.cat((hx5dup,hx4),1))\n",
    "        hx4dup = _upsample_like(hx4d,hx3)\n",
    "\n",
    "        hx3d = self.stage3d(torch.cat((hx4dup,hx3),1))\n",
    "        hx3dup = _upsample_like(hx3d,hx2)\n",
    "\n",
    "        hx2d = self.stage2d(torch.cat((hx3dup,hx2),1))\n",
    "        hx2dup = _upsample_like(hx2d,hx1)\n",
    "\n",
    "        hx1d = self.stage1d(torch.cat((hx2dup,hx1),1))\n",
    "\n",
    "\n",
    "        #side output\n",
    "        d1 = self.side1(hx1d)\n",
    "\n",
    "        d2 = self.side2(hx2d)\n",
    "        d2 = _upsample_like(d2,d1)\n",
    "\n",
    "        d3 = self.side3(hx3d)\n",
    "        d3 = _upsample_like(d3,d1)\n",
    "\n",
    "        d4 = self.side4(hx4d)\n",
    "        d4 = _upsample_like(d4,d1)\n",
    "\n",
    "        d5 = self.side5(hx5d)\n",
    "        d5 = _upsample_like(d5,d1)\n",
    "\n",
    "        d6 = self.side6(hx6)\n",
    "        d6 = _upsample_like(d6,d1)\n",
    "\n",
    "        d0 = self.outconv(torch.cat((d1,d2,d3,d4,d5,d6),1))\n",
    "\n",
    "        return F.sigmoid(d0), F.sigmoid(d1), F.sigmoid(d2), F.sigmoid(d3), F.sigmoid(d4), F.sigmoid(d5), F.sigmoid(d6)\n",
    "    \n",
    "bce_loss = nn.BCELoss(size_average=True)\n",
    "\n",
    "def muti_bce_loss_fusion(yhat, labels_v):\n",
    "    d0, d1, d2, d3, d4, d5, d6 = yhat\n",
    "    loss0 = bce_loss(d0,labels_v)\n",
    "    loss1 = bce_loss(d1,labels_v)\n",
    "    loss2 = bce_loss(d2,labels_v)\n",
    "    loss3 = bce_loss(d3,labels_v)\n",
    "    loss4 = bce_loss(d4,labels_v)\n",
    "    loss5 = bce_loss(d5,labels_v)\n",
    "    loss6 = bce_loss(d6,labels_v)\n",
    "\n",
    "    loss = loss0 + loss1 + loss2 + loss3 + loss4 + loss5 + loss6\n",
    "    # print(\"l0: %3f, l1: %3f, l2: %3f, l3: %3f, l4: %3f, l5: %3f, l6: %3f\\n\"%(loss0.data.item(),loss1.data.item(),loss2.data.item(),loss3.data.item(),loss4.data.item(),loss5.data.item(),loss6.data.item()))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def bn2group(module):\n",
    "    num_groups = 16 # hyper_parameter of GroupNorm\n",
    "    # num_groups = 8 # hyper_parameter of GroupNorm\n",
    "    module_output = module\n",
    "    if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):\n",
    "        if module.num_features/num_groups <1:\n",
    "            module_output = torch.nn.GroupNorm(1,\n",
    "                                           module.num_features,\n",
    "                                           module.eps, \n",
    "                                           module.affine,\n",
    "                                          )\n",
    "        else:\n",
    "            module_output = torch.nn.GroupNorm(num_groups,\n",
    "                               module.num_features,\n",
    "                               module.eps, \n",
    "                               module.affine,\n",
    "                                          )\n",
    "\n",
    "        if module.affine:\n",
    "            with torch.no_grad():\n",
    "                module_output.weight = module.weight\n",
    "                module_output.bias = module.bias\n",
    "        module_output.running_mean = module.running_mean\n",
    "        module_output.running_var = module.running_var\n",
    "        module_output.num_batches_tracked = module.num_batches_tracked\n",
    "        \n",
    "        if hasattr(module, \"qconfig\"):\n",
    "            module_output.qconfig = module.qconfig\n",
    "\n",
    "    for name, child in module.named_children():\n",
    "        module_output.add_module(name, bn2group(child))\n",
    "\n",
    "    del module\n",
    "    return module_output\n",
    "\n",
    "net = U2NET(in_ch=3,out_ch=5)\n",
    "net = bn2group(net)\n",
    "# net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import *\n",
    "\n",
    "class Segmentor(pl.LightningModule):\n",
    "    def __init__(self, network, lossfn, metricfn, experiment_name):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = network\n",
    "        self.lossfn = lossfn\n",
    "        self.metricfn = metricfn\n",
    "        self.experiment_name = experiment_name\n",
    "        self.best_val_loss_epoch = np.inf            \n",
    "        self.best_valid_epoch = 0\n",
    "        \n",
    "        if isinstance(lossfn,list) and isinstance(metricfn,list):\n",
    "            assert len(lossfn) == len(metricfn)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "        \n",
    "    def pipeline(self, batch, sw=False, plot=False):\n",
    "        x, y = batch['image'].float(), batch['seg'].float()\n",
    "        if not sw:\n",
    "            yhat = self.net(x)\n",
    "            loss = muti_bce_loss_fusion(yhat, y)\n",
    "            yhat = yhat[0]\n",
    "            metric = torch.mean(metricfn(yhat,y,))\n",
    "        else:\n",
    "            def predictor(x, return_idx = 0): # in case of prediction is type of list\n",
    "                result = self.net(x)\n",
    "                if isinstance(result, list) or isinstance(result, tuple):\n",
    "                    return result[return_idx]\n",
    "                else:\n",
    "                    return resul\n",
    "            yhat = sliding_window_inference(x, roi_size=(512,1024), sw_batch_size=2, predictor=predictor)\n",
    "            loss = bce_loss(yhat,y)\n",
    "            metric = torch.mean(metricfn(yhat,y,))\n",
    "            # from scipy.ndimage import label, binary_closing\n",
    "            # for i in range(yhat.shape[1]):\n",
    "            #     for j in range(yhat.shape[1]):\n",
    "            #         yhat[i,j] = binary_closing(yhat[i,j], structure=np.ones(13,13)) # closing of R-peak\n",
    "    \n",
    "        if plot:\n",
    "            for idx in range(len(x)):\n",
    "                visualize(image=x[idx].cpu(), mask1=y[idx,0].cpu().detach().numpy(), mask2=y[idx,1].cpu().detach().numpy(), mask3=y[idx,2].cpu().detach().numpy(), mask4=y[idx,3].cpu().detach().numpy(), mask5=y[idx,4].cpu().detach().numpy())\n",
    "                visualize(image=x[idx].cpu(), mask1=yhat[idx,0].cpu().detach().numpy(), mask2=yhat[idx,1].cpu().detach().numpy(), mask3=yhat[idx,2].cpu().detach().numpy(), mask4=yhat[idx,3].cpu().detach().numpy(), mask5=yhat[idx,4].cpu().detach().numpy())\n",
    "        return loss, metric\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.net.parameters(), lr=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=.9)\n",
    "        # return optimizer\n",
    "        return {'optimizer': optimizer,\n",
    "                'lr_scheduler': {'scheduler': scheduler, 'monitor': 'val_loss'}}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, metric = self.pipeline(batch, sw=False, plot=False)\n",
    "        self.log('loss', loss, on_step=True, on_epoch=True, prog_bar=True,)\n",
    "        if isinstance(metric,list):\n",
    "            self.log('f1', metric[0], on_step=True, on_epoch=True, prog_bar=True,)\n",
    "            self.log('acc', metric[1], on_step=True, on_epoch=True, prog_bar=True,)\n",
    "        else:\n",
    "            self.log('f1', metric, on_step=True, on_epoch=True, prog_bar=True,)            \n",
    "        return loss\n",
    "      \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, metric = self.pipeline(batch, sw=True, plot=False)\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True,)\n",
    "        if isinstance(metric,list):\n",
    "            self.log('val_f1', metric[0], on_epoch=True, prog_bar=True,)\n",
    "            self.log('val_acc', metric[1], on_epoch=True, prog_bar=True,)\n",
    "        else:\n",
    "            self.log('val_f1', metric, on_epoch=True, prog_bar=True,)            \n",
    "        return {\"val_loss\":loss}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, metric = self.pipeline(batch, sw=True, plot=False)\n",
    "        self.log('test_loss', loss, on_step=True, on_epoch=True, prog_bar=True,)\n",
    "        if isinstance(metric,list):\n",
    "            self.log('test_f1', metric[0], on_epoch=True, prog_bar=True,)\n",
    "            self.log('test_acc', metric[1], on_epoch=True, prog_bar=True,)\n",
    "        else:\n",
    "            self.log('test_f1', metric, on_epoch=True, prog_bar=True,)            \n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        val_losses = []\n",
    "        for output in outputs:\n",
    "            val_losses.append(output[\"val_loss\"].cpu().detach().numpy())\n",
    "        val_loss_epoch = np.mean(val_losses)\n",
    "        # self.log('val_loss_epoch', val_loss_epoch)\n",
    "        \n",
    "        if val_loss_epoch < self.best_val_loss_epoch and self.current_epoch>0:\n",
    "            self.best_valid_epoch = self.current_epoch\n",
    "            self.best_val_loss_epoch = val_loss_epoch             \n",
    "        print(\n",
    "            f\"current epoch: {self.current_epoch}, \"\n",
    "            f\"current epoch val_loss: {val_loss_epoch:.4f}, \"\n",
    "            f\"best epoch val_loss: {self.best_val_loss_epoch:.4f}, \"\n",
    "            f\"at epoch: {self.best_valid_epoch}, \" \n",
    "        )\n",
    "                \n",
    "import torchmetrics\n",
    "lossfn = monai.losses.DiceFocalLoss(to_onehot_y=False)\n",
    "\n",
    "# metricfn = torchmetrics.functional.dice_score\n",
    "metricfn = torchmetrics.functional.dice_score\n",
    "# metricfn = sklearn.metrics.f1_score\n",
    "\n",
    "# model\n",
    "experiment_name = 'vit'\n",
    "model = Segmentor(network=net, lossfn=lossfn, metricfn=metricfn, experiment_name=experiment_name,)\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=10, verbose=False, mode=\"min\")\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=f\"weight/{experiment_name}/\", save_top_k=1, monitor=\"val_loss_epoch\")\n",
    "trainer = pl.Trainer(gpus=-1, strategy='dp', precision=32, max_epochs=1000, callbacks=[checkpoint_callback, early_stop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_from_checkpoint(network=net, lossfn=lossfn, metricfn=metricfn, experiment_name=experiment_name,checkpoint_path='weight/U2NET/epoch=9-step=229.ckpt')\n",
    "model.load_from_checkpoint(network=net, lossfn=lossfn, metricfn=metricfn, experiment_name=experiment_name,checkpoint_path='weight/U2NET/epoch=18-step=436.ckpt')\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install pydicom\n",
    "# # import pydicom\n",
    "# # import nibabel as nib\n",
    "# import cv2, skimage\n",
    "# from skimage.morphology import disk\n",
    "# from scipy.spatial.distance import directed_hausdorff\n",
    "# from sklearn.metrics import f1_score\n",
    "# import monai\n",
    "\n",
    "# metricfn = f1_score\n",
    "# metricfn = directed_hausdorff\n",
    "\n",
    "# def mask2contour(mask):\n",
    "#     mask = mask.astype(np.uint8)\n",
    "#     contour = cv2.Laplacian(mask,cv2.CV_8U,ksize=3)\n",
    "#     result = skimage.morphology.dilation(contour,disk(4))\n",
    "#     result = skimage.morphology.erosion(result,disk(4))\n",
    "#     result[result!=0] = 1\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pydicom\n",
    "# import nibabel as nib\n",
    "import cv2, skimage\n",
    "from skimage.morphology import disk\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from sklearn.metrics import f1_score\n",
    "from monai.transforms import LabelToContour\n",
    "\n",
    "metricfn = f1_score\n",
    "metricfn = directed_hausdorff\n",
    "\n",
    "def mask2contour(mask):\n",
    "    mask = mask.astype(np.uint8)\n",
    "    contour = cv2.Laplacian(mask,cv2.CV_8U,ksize=3)\n",
    "    result = skimage.morphology.dilation(contour,disk(4))\n",
    "    result = skimage.morphology.erosion(result,disk(4))\n",
    "    result[result!=0] = 1\n",
    "    return result\n",
    "\n",
    "def postprocess(mask):\n",
    "    post_process = monai.transforms.KeepLargestConnectedComponent(applied_labels=[1], connectivity=2,)\n",
    "    return post_process(torch.tensor(mask.round()))\n",
    "\n",
    "postProcess = Compose(\n",
    "    [\n",
    "        KeepLargestConnectedComponent(applied_labels=[1], is_onehot=True, connectivity=2),\n",
    "        FillHoles(applied_labels=[1])\n",
    "    ]\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "from skimage.measure import label   \n",
    "\n",
    "def getLargestCC(segmentation):\n",
    "    labels = label(segmentation)\n",
    "    assert( labels.max() != 0 ) # assume at least 1 CC\n",
    "    largestCC = labels == np.argmax(np.bincount(labels.flat)[1:])+1\n",
    "    return largestCC.astype(np.uint8)\n",
    "\n",
    "def test(loader, plot=False):\n",
    "    net.eval()\n",
    "    \n",
    "    xs = list()\n",
    "    ys = list()\n",
    "    yhats = list()\n",
    "    fnames = list()\n",
    "    DSCs = list()\n",
    "    HDs = list()\n",
    "    angle_PCA_ys = list()\n",
    "    angle_PCA_yhats = list()\n",
    "    angle_inertia_ys = list()\n",
    "    angle_inertia_yhats = list()\n",
    "    angle_ellipse_ys = list()\n",
    "    angle_ellipse_yhats = list()\n",
    "    \n",
    "    for idx, batch in tqdm(enumerate(loader)):\n",
    "        x, y_seg, fname = batch['image'], batch['seg'], batch['fname']\n",
    "        print(fname)\n",
    "        xs.append(x)\n",
    "        ys.append(y_seg)\n",
    "        fnames.append(fname)\n",
    "        \n",
    "        x = x.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            yhat = net(x)\n",
    "            yhat = yhat[0] # only for U2NET\n",
    "            \n",
    "            def predictor(x, return_idx = 0): # in case of prediction is type of list\n",
    "                result = model(x)\n",
    "                if isinstance(result, list) or isinstance(result, tuple):\n",
    "                    return result[return_idx]\n",
    "                else:\n",
    "                    return result\n",
    "                \n",
    "            # print(resize_toNet(x).shape)\n",
    "            # plt.imshow(resize_toNet(x)[0,0].cpu().detach())\n",
    "            # plt.show()\n",
    "                \n",
    "            # yhat = sliding_window_inference(resize_toNet(x), roi_size=(512,1024), sw_batch_size=2, predictor=predictor)\n",
    "            yhat = sliding_window_inference(x, roi_size=(512, 1024), sw_batch_size=2, predictor=predictor)\n",
    "            # yhat = resize_toOriginal(yhat)\n",
    "            # yhat = pad(yhat)\n",
    "            # yhat = yhat.cpu().detach().numpy()\n",
    "            # yhat[0] = postProcess(yhat[0].round())\n",
    "            yhat = yhat.cpu().detach().numpy()\n",
    "            # yhat = postprocess(yhat)\n",
    "            \n",
    "            temp_DSC = np.zeros((5))\n",
    "            temp_HD = np.zeros((5))\n",
    "            temp_angle_PCA_y = np.zeros((5))\n",
    "            temp_angle_PCA_yhat = np.zeros((5))\n",
    "            temp_angle_inertia_y = np.zeros((5))\n",
    "            temp_angle_inertia_yhat = np.zeros((5))\n",
    "            temp_angle_ellipse_y = np.zeros((5))\n",
    "            temp_angle_ellipse_yhat = np.zeros((5))\n",
    "            \n",
    "            for idx_class in range(yhat.shape[1]):\n",
    "                # yhat = getLargestCC(yhat)\n",
    "                yhat[0][idx_class] = getLargestCC(yhat[0][idx_class].round())\n",
    "                DSC = f1_score(y_seg[0][idx_class].flatten(), yhat[0][idx_class].flatten())\n",
    "                HD = directed_hausdorff(y_seg[0][idx_class], yhat[0][idx_class])[0]\n",
    "                \n",
    "                temp_DSC[idx_class] = DSC\n",
    "                temp_HD[idx_class] = HD\n",
    "    \n",
    "                head = head_direction(y_seg[0][0])\n",
    "                major = True if idx_class <4 else False     \n",
    "                clockwise_plus = True if idx_class==0 else False\n",
    "            \n",
    "                x0,y0,angle = PCA_axis(y_seg[0][idx_class].numpy(),major)\n",
    "                temp_angle_PCA_y[idx_class] = refine_degree(angle,head,clockwise_plus=clockwise_plus)\n",
    "\n",
    "                x0,y0,angle = PCA_axis(yhat[0][idx_class].round(),major)\n",
    "                temp_angle_PCA_yhat[idx_class] = refine_degree(angle,head,clockwise_plus=clockwise_plus)\n",
    "                                                         \n",
    "                x0,y0,angle = ellipse_axis(y_seg[0][idx_class].numpy(),major)\n",
    "                temp_angle_ellipse_y[idx_class] = refine_degree(angle,head,clockwise_plus=clockwise_plus)\n",
    "    \n",
    "                x0,y0,angle = ellipse_axis(yhat[0][idx_class].round(),major)\n",
    "                temp_angle_ellipse_yhat[idx_class] = refine_degree(angle,head,clockwise_plus=clockwise_plus)\n",
    "                                                         \n",
    "                x0,y0,angle = inertia_axis(y_seg[0][idx_class].numpy(),major)\n",
    "                temp_angle_inertia_y[idx_class] = refine_degree(angle,head,clockwise_plus=clockwise_plus)\n",
    "                \n",
    "                x0,y0,angle = inertia_axis(yhat[0][idx_class].round(),major)\n",
    "                temp_angle_inertia_yhat[idx_class] = refine_degree(angle,head,clockwise_plus=clockwise_plus)\n",
    "            \n",
    "            if plot:\n",
    "                visualize(\n",
    "                    x=x[0].cpu().detach(), \n",
    "                    y1 =y_seg[0][0].round(),\n",
    "                    y2 =y_seg[0][1].round(),\n",
    "                    y3 =y_seg[0][2].round(),\n",
    "                    y4 =y_seg[0][3].round(),\n",
    "                    y5 =y_seg[0][4].round(),\n",
    "                )\n",
    "                visualize(\n",
    "                    x=x[0].cpu().detach(), \n",
    "                    yhat1 =yhat[0][0].round(),\n",
    "                    yhat2 =yhat[0][1].round(),\n",
    "                    yhat3 =yhat[0][2].round(),\n",
    "                    yhat4 =yhat[0][3].round(),\n",
    "                    yhat5 =yhat[0][4].round(),\n",
    "                )\n",
    "                \n",
    "            yhats.append(yhat)\n",
    "            DSCs.append(temp_DSC)\n",
    "            HDs.append(temp_HD)\n",
    "            angle_PCA_ys.append(temp_angle_PCA_y)\n",
    "            angle_PCA_yhats.append(temp_angle_PCA_yhat)\n",
    "            angle_inertia_ys.append(temp_angle_inertia_y)\n",
    "            angle_inertia_yhats.append(temp_angle_inertia_yhat)\n",
    "            angle_ellipse_ys.append(temp_angle_ellipse_y)\n",
    "            angle_ellipse_yhats.append(temp_angle_ellipse_yhat)\n",
    "            \n",
    "    DSCs = np.array(DSCs)\n",
    "    HDs = np.array(HDs)\n",
    "    angle_PCA_ys = np.array(angle_PCA_ys)\n",
    "    angle_PCA_yhats = np.array(angle_PCA_yhats)\n",
    "    angle_inertia_ys = np.array(angle_inertia_ys)\n",
    "    angle_inertia_yhats = np.array(angle_inertia_yhats)\n",
    "    angle_ellipse_ys = np.array(angle_ellipse_ys)\n",
    "    angle_ellipse_yhats = np.array(angle_ellipse_yhats)\n",
    "    \n",
    "    return {'xs':xs, 'ys':ys, 'yhats':yhats, 'fnames':fnames, 'DSCs':DSCs, 'HDs':HDs, \n",
    "            'angle_PCA_ys':angle_PCA_ys, 'angle_PCA_yhats':angle_PCA_yhats,\n",
    "            'angle_inertia_ys':angle_inertia_ys, 'angle_inertia_yhats':angle_inertia_yhats,\n",
    "            'angle_ellipse_ys':angle_ellipse_ys, 'angle_ellipse_yhats':angle_ellipse_yhats}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train 각도만 추출\n",
    "# result = test(train_loader)\n",
    "# # STAGE='ActiveLearning'\n",
    "# # STAGE='NonActiveLearning'\n",
    "\n",
    "# xs = result['xs']\n",
    "# ys = result['ys']\n",
    "# fnames = result['fnames']\n",
    "\n",
    "# angle_PCA_ys = result['angle_PCA_ys']\n",
    "# angle_inertia_ys = result['angle_inertia_ys']\n",
    "# angle_ellipse_ys = result['angle_ellipse_ys']\n",
    "\n",
    "# df = pd.DataFrame([fnames,angle_PCA_ys[:,0],angle_PCA_ys[:,1],angle_PCA_ys[:,2],angle_PCA_ys[:,3],angle_PCA_ys[:,4],\n",
    "#                    angle_inertia_ys[:,0],angle_inertia_ys[:,1],angle_inertia_ys[:,2],angle_inertia_ys[:,3],angle_inertia_ys[:,4],\n",
    "#                    angle_ellipse_ys[:,0],angle_ellipse_ys[:,1],angle_ellipse_ys[:,2],angle_ellipse_ys[:,3],angle_ellipse_ys[:,4]],\n",
    "#                   ['fnames',STAGE+'_angle_PCA_y_1',STAGE+'_angle_PCA_y_2',STAGE+'_angle_PCA_y_3',STAGE+'_angle_PCA_y_4',STAGE+'_angle_PCA_y_5',\n",
    "#                    STAGE+'_angle_inertia_y_1',STAGE+'_angle_inertia_y_2',STAGE+'_angle_inertia_y_3',STAGE+'_angle_inertia_y_4',STAGE+'_angle_inertia_y_5',\n",
    "#                    STAGE+'_angle_ellipse_y_1',STAGE+'_angle_ellipse_y_2',STAGE+'_angle_ellipse_y_3',STAGE+'_angle_ellipse_y_4',STAGE+'_angle_ellipse_y_5'])\n",
    "# df = df.T\n",
    "# df\n",
    "# # df.to_csv(STAGE+'_metrics_train.csv',index=False)\n",
    "# df.to_csv('20220428_DrRyu_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import trange\n",
    "\n",
    "# # Afiles = natsort.natsorted(glob.glob('ExternalObserver/SMR/x_test/*.png'))\n",
    "# # Afiles = natsort.natsorted(glob.glob('ActiveLearning/4th/x_test/*.png'))\n",
    "# Afiles = natsort.natsorted(glob.glob('ActiveLearning/4th/x_test/*.png'))[:50] + natsort.natsorted(glob.glob('ExternalObserver/SMR/x_test/*.png'))\n",
    "\n",
    "# Gfiles = natsort.natsorted(glob.glob('ExternalObserver/GP1/x_test/*.png'))\n",
    "# gfiles = natsort.natsorted(glob.glob('ExternalObserver/GP1/y_test/*.npy'))\n",
    "\n",
    "# # Gfiles = natsort.natsorted(glob.glob('ExternalObserver/GP2/x_test/*.png'))\n",
    "# # gfiles = natsort.natsorted(glob.glob('ExternalObserver/GP2/y_test/*.npy'))\n",
    "\n",
    "# # Gfiles = natsort.natsorted(glob.glob('ExternalObserver/OS/x_test/*.png'))\n",
    "# # gfiles = natsort.natsorted(glob.glob('ExternalObserver/OS/y_test/*.npy'))\n",
    "\n",
    "# print(len(Afiles), len(Gfiles))\n",
    "# for idx in trange(100):\n",
    "#     A = cv2.imread(Afiles[idx],0)\n",
    "#     G = cv2.imread(Gfiles[idx],0)\n",
    "#     h = G.shape[0] - A.shape[0]\n",
    "#     w = A.shape[1] - G.shape[1]\n",
    "#     print(A.shape,G.shape,h,w)\n",
    "    \n",
    "#     # file = Gfiles[idx]\n",
    "#     # G = G[h:].astype(np.uint8)\n",
    "#     # cv2.imwrite(file,G)\n",
    "    \n",
    "# #     file = gfiles[idx]\n",
    "# #     g = np.load(file)\n",
    "# #     g = g[h:].astype(np.uint8)\n",
    "# #     np.save(file,g)\n",
    " \n",
    "#     # print(A.shape, G.shape, g.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # iterator = iter(test_loader)\n",
    "# # iteratorGP1 = iter(testGP1_loader)\n",
    "# # iteratorGP2 = iter(testGP2_loader)\n",
    "# # iteratorOS = iter(testOS_loader)\n",
    "\n",
    "def DSC_HD(loaderGT,loaderHAT,STAGE):\n",
    "    \n",
    "    DSCs = list()\n",
    "    HDs = list()\n",
    "    fnames = list()\n",
    "    \n",
    "    iterator = iter(loaderGT)\n",
    "    iterator_ = iter(loaderHAT)\n",
    "    \n",
    "    for idx in trange(len(loaderGT)):        \n",
    "        batch = iterator.next()  \n",
    "        batch_ = iterator_.next()  \n",
    "\n",
    "        x, y, fname = batch['image'], batch['seg'], batch['fname']\n",
    "        x_, y_, fname_ = batch_['image'], batch_['seg'], batch_['fname']\n",
    "        \n",
    "        temp_DSC = np.zeros((5))\n",
    "        temp_HD = np.zeros((5))\n",
    "\n",
    "        for idx_class in range(y.shape[1]):\n",
    "            # print(idx, y.shape,y_.shape, torch.unique(y[0][idx_class]),torch.unique(y_[0][idx_class]))\n",
    "            # plt.imshow(y[0][idx_class])\n",
    "            # plt.show()\n",
    "            # plt.imshow(y_[0][idx_class])\n",
    "            # plt.show()\n",
    "            DSC = f1_score(y[0][idx_class].flatten(), y_[0][idx_class].flatten())\n",
    "            HD = directed_hausdorff(y[0][idx_class], y_[0][idx_class])[0]\n",
    "            \n",
    "            temp_DSC[idx_class] = DSC\n",
    "            temp_HD[idx_class] = HD\n",
    "        \n",
    "        DSCs.append(temp_DSC)\n",
    "        HDs.append(temp_HD)\n",
    "        fnames.append(fname)\n",
    "    \n",
    "    DSCs = np.array(DSCs)\n",
    "    HDs = np.array(HDs)\n",
    "    df = pd.DataFrame([fnames,DSCs[:,0],DSCs[:,1],DSCs[:,2],DSCs[:,3],DSCs[:,4],HDs[:,0],HDs[:,1],HDs[:,2],HDs[:,3],HDs[:,4]],\n",
    "                      ['fnames',STAGE+'_DSC_1',STAGE+'_DSC_2',STAGE+'_DSC_3',STAGE+'_DSC_4',STAGE+'_DSC_5',STAGE+'_HD_1',STAGE+'_HD_2',STAGE+'_HD_3',STAGE+'_HD_4',STAGE+'_HD_5'])\n",
    "    df = df.T\n",
    "        \n",
    "    return df\n",
    "\n",
    "STAGE = 'OS'\n",
    "df = DSC_HD(test_loader,testOS_loader,STAGE)\n",
    "df.to_csv(f'20220502_{STAGE}_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test(test_loader, plot=True)\n",
    "\n",
    "STAGE = 'DR_Ryu'\n",
    "# STAGE = 'GP1'\n",
    "# STAGE = 'GP2'\n",
    "# STAGE = 'OS'\n",
    "\n",
    "xs = result['xs']\n",
    "ys = result['ys']\n",
    "yhats = result['yhats']\n",
    "fnames = result['fnames']\n",
    "DSCs = result['DSCs']\n",
    "HDs = result['HDs']\n",
    "                                                            \n",
    "angle_PCA_ys = result['angle_PCA_ys']\n",
    "angle_PCA_yhats = result['angle_PCA_yhats']\n",
    "angle_inertia_ys = result['angle_inertia_ys']\n",
    "angle_inertia_yhats = result['angle_inertia_yhats']\n",
    "angle_ellipse_ys = result['angle_ellipse_ys']\n",
    "angle_ellipse_yhats = result['angle_ellipse_yhats']\n",
    "# np.save('{}/result.npy'.format(STAGE), result)\n",
    "\n",
    "print(len(xs),len(ys),len(yhats),len(fnames),len(DSCs),len(HDs))\n",
    "\n",
    "for idx in range(DSCs.shape[1]):\n",
    "    print('class {:2d}, DSC {:3.3f}, HD {:3.3f}'.format(idx+1,np.mean(DSCs[:,idx]),np.mean(HDs[:,idx])))\n",
    "\n",
    "df = pd.DataFrame([fnames,DSCs[:,0],DSCs[:,1],DSCs[:,2],DSCs[:,3],DSCs[:,4],HDs[:,0],HDs[:,1],HDs[:,2],HDs[:,3],HDs[:,4],angle_PCA_ys[:,0],angle_PCA_ys[:,1],angle_PCA_ys[:,2],angle_PCA_ys[:,3],angle_PCA_ys[:,4],angle_PCA_yhats[:,0],angle_PCA_yhats[:,1],angle_PCA_yhats[:,2],angle_PCA_yhats[:,3],angle_PCA_yhats[:,4],angle_inertia_ys[:,0],angle_inertia_ys[:,1],angle_inertia_ys[:,2],angle_inertia_ys[:,3],angle_inertia_ys[:,4],angle_inertia_yhats[:,0],angle_inertia_yhats[:,1],angle_inertia_yhats[:,2],angle_inertia_yhats[:,3],angle_inertia_yhats[:,4],angle_ellipse_ys[:,0],angle_ellipse_ys[:,1],angle_ellipse_ys[:,2],angle_ellipse_ys[:,3],angle_ellipse_ys[:,4],angle_ellipse_yhats[:,0],angle_ellipse_yhats[:,1],angle_ellipse_yhats[:,2],angle_ellipse_yhats[:,3],angle_ellipse_yhats[:,4]],['fnames',STAGE+'_DSC_1',STAGE+'_DSC_2',STAGE+'_DSC_3',STAGE+'_DSC_4',STAGE+'_DSC_5',STAGE+'_HD_1',STAGE+'_HD_2',STAGE+'_HD_3',STAGE+'_HD_4',STAGE+'_HD_5',STAGE+'_angle_PCA_y_1',STAGE+'_angle_PCA_y_2',STAGE+'_angle_PCA_y_3',STAGE+'_angle_PCA_y_4',STAGE+'_angle_PCA_y_5',STAGE+'_angle_PCA_yhat_1',STAGE+'_angle_PCA_yhat_2',STAGE+'_angle_PCA_yhat_3',STAGE+'_angle_PCA_yhat_4',STAGE+'_angle_PCA_yhat_5',STAGE+'_angle_inertia_y_1',STAGE+'_angle_inertia_y_2',STAGE+'_angle_inertia_y_3',STAGE+'_angle_inertia_y_4',STAGE+'_angle_inertia_y_5',STAGE+'_angle_inertia_yhat_1',STAGE+'_angle_inertia_yhat_2',STAGE+'_angle_inertia_yhat_3',STAGE+'_angle_inertia_yhat_4',STAGE+'_angle_inertia_yhat_5',STAGE+'_angle_ellipse_y_1',STAGE+'_angle_ellipse_y_2',STAGE+'_angle_ellipse_y_3',STAGE+'_angle_ellipse_y_4',STAGE+'_angle_ellipse_y_5',STAGE+'_angle_ellipse_yhat_1',STAGE+'_angle_ellipse_yhat_2',STAGE+'_angle_ellipse_yhat_3',STAGE+'_angle_ellipse_yhat_4',STAGE+'_angle_ellipse_yhat_5'])\n",
    "df = df.T\n",
    "# df.to_csv('{}/metrics.csv'.format(STAGE),index=False)\n",
    "# df.to_csv('20220428_DrRyu_test.csv',index=False)\n",
    "\n",
    "# df.to_csv(f'20220428_{STAGE}_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test(etest_loader,plot=True)\n",
    "\n",
    "# STAGE = 'DR_Ryu_LERA'\n",
    "STAGE = 'GP1_LERA'\n",
    "\n",
    "xs = result['xs']\n",
    "ys = result['ys']\n",
    "yhats = result['yhats']\n",
    "fnames = result['fnames']\n",
    "DSCs = result['DSCs']\n",
    "HDs = result['HDs']\n",
    "                                                            \n",
    "angle_PCA_ys = result['angle_PCA_ys']\n",
    "angle_PCA_yhats = result['angle_PCA_yhats']\n",
    "angle_inertia_ys = result['angle_inertia_ys']\n",
    "angle_inertia_yhats = result['angle_inertia_yhats']\n",
    "angle_ellipse_ys = result['angle_ellipse_ys']\n",
    "angle_ellipse_yhats = result['angle_ellipse_yhats']\n",
    "# np.save('{}/result.npy'.format(STAGE), result)\n",
    "\n",
    "print(len(xs),len(ys),len(yhats),len(fnames),len(DSCs),len(HDs))\n",
    "\n",
    "for idx in range(DSCs.shape[1]):\n",
    "    print('class {:2d}, DSC {:3.3f}, HD {:3.3f}'.format(idx+1,np.mean(DSCs[:,idx]),np.mean(HDs[:,idx])))\n",
    "\n",
    "df = pd.DataFrame([fnames,DSCs[:,0],DSCs[:,1],DSCs[:,2],DSCs[:,3],DSCs[:,4],HDs[:,0],HDs[:,1],HDs[:,2],HDs[:,3],HDs[:,4],angle_PCA_ys[:,0],angle_PCA_ys[:,1],angle_PCA_ys[:,2],angle_PCA_ys[:,3],angle_PCA_ys[:,4],angle_PCA_yhats[:,0],angle_PCA_yhats[:,1],angle_PCA_yhats[:,2],angle_PCA_yhats[:,3],angle_PCA_yhats[:,4],angle_inertia_ys[:,0],angle_inertia_ys[:,1],angle_inertia_ys[:,2],angle_inertia_ys[:,3],angle_inertia_ys[:,4],angle_inertia_yhats[:,0],angle_inertia_yhats[:,1],angle_inertia_yhats[:,2],angle_inertia_yhats[:,3],angle_inertia_yhats[:,4],angle_ellipse_ys[:,0],angle_ellipse_ys[:,1],angle_ellipse_ys[:,2],angle_ellipse_ys[:,3],angle_ellipse_ys[:,4],angle_ellipse_yhats[:,0],angle_ellipse_yhats[:,1],angle_ellipse_yhats[:,2],angle_ellipse_yhats[:,3],angle_ellipse_yhats[:,4]],['fnames',STAGE+'_DSC_1',STAGE+'_DSC_2',STAGE+'_DSC_3',STAGE+'_DSC_4',STAGE+'_DSC_5',STAGE+'_HD_1',STAGE+'_HD_2',STAGE+'_HD_3',STAGE+'_HD_4',STAGE+'_HD_5',STAGE+'_angle_PCA_y_1',STAGE+'_angle_PCA_y_2',STAGE+'_angle_PCA_y_3',STAGE+'_angle_PCA_y_4',STAGE+'_angle_PCA_y_5',STAGE+'_angle_PCA_yhat_1',STAGE+'_angle_PCA_yhat_2',STAGE+'_angle_PCA_yhat_3',STAGE+'_angle_PCA_yhat_4',STAGE+'_angle_PCA_yhat_5',STAGE+'_angle_inertia_y_1',STAGE+'_angle_inertia_y_2',STAGE+'_angle_inertia_y_3',STAGE+'_angle_inertia_y_4',STAGE+'_angle_inertia_y_5',STAGE+'_angle_inertia_yhat_1',STAGE+'_angle_inertia_yhat_2',STAGE+'_angle_inertia_yhat_3',STAGE+'_angle_inertia_yhat_4',STAGE+'_angle_inertia_yhat_5',STAGE+'_angle_ellipse_y_1',STAGE+'_angle_ellipse_y_2',STAGE+'_angle_ellipse_y_3',STAGE+'_angle_ellipse_y_4',STAGE+'_angle_ellipse_y_5',STAGE+'_angle_ellipse_yhat_1',STAGE+'_angle_ellipse_yhat_2',STAGE+'_angle_ellipse_yhat_3',STAGE+'_angle_ellipse_yhat_4',STAGE+'_angle_ellipse_yhat_5'])\n",
    "df = df.T\n",
    "# df.to_csv(f'20220428_{STAGE}_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test(etest2_loader,plot=True)\n",
    "STAGE = 'DR_Ryu_AMC'\n",
    "\n",
    "xs = result['xs']\n",
    "ys = result['ys']\n",
    "yhats = result['yhats']\n",
    "fnames = result['fnames']\n",
    "DSCs = result['DSCs']\n",
    "HDs = result['HDs']\n",
    "                                                            \n",
    "angle_PCA_ys = result['angle_PCA_ys']\n",
    "angle_PCA_yhats = result['angle_PCA_yhats']\n",
    "angle_inertia_ys = result['angle_inertia_ys']\n",
    "angle_inertia_yhats = result['angle_inertia_yhats']\n",
    "angle_ellipse_ys = result['angle_ellipse_ys']\n",
    "angle_ellipse_yhats = result['angle_ellipse_yhats']\n",
    "# np.save('{}/result.npy'.format(STAGE), result)\n",
    "\n",
    "print(len(xs),len(ys),len(yhats),len(fnames),len(DSCs),len(HDs))\n",
    "\n",
    "for idx in range(DSCs.shape[1]):\n",
    "    print('class {:2d}, DSC {:3.3f}, HD {:3.3f}'.format(idx+1,np.mean(DSCs[:,idx]),np.mean(HDs[:,idx])))\n",
    "\n",
    "df = pd.DataFrame([fnames,DSCs[:,0],DSCs[:,1],DSCs[:,2],DSCs[:,3],DSCs[:,4],HDs[:,0],HDs[:,1],HDs[:,2],HDs[:,3],HDs[:,4],angle_PCA_ys[:,0],angle_PCA_ys[:,1],angle_PCA_ys[:,2],angle_PCA_ys[:,3],angle_PCA_ys[:,4],angle_PCA_yhats[:,0],angle_PCA_yhats[:,1],angle_PCA_yhats[:,2],angle_PCA_yhats[:,3],angle_PCA_yhats[:,4],angle_inertia_ys[:,0],angle_inertia_ys[:,1],angle_inertia_ys[:,2],angle_inertia_ys[:,3],angle_inertia_ys[:,4],angle_inertia_yhats[:,0],angle_inertia_yhats[:,1],angle_inertia_yhats[:,2],angle_inertia_yhats[:,3],angle_inertia_yhats[:,4],angle_ellipse_ys[:,0],angle_ellipse_ys[:,1],angle_ellipse_ys[:,2],angle_ellipse_ys[:,3],angle_ellipse_ys[:,4],angle_ellipse_yhats[:,0],angle_ellipse_yhats[:,1],angle_ellipse_yhats[:,2],angle_ellipse_yhats[:,3],angle_ellipse_yhats[:,4]],['fnames',STAGE+'_DSC_1',STAGE+'_DSC_2',STAGE+'_DSC_3',STAGE+'_DSC_4',STAGE+'_DSC_5',STAGE+'_HD_1',STAGE+'_HD_2',STAGE+'_HD_3',STAGE+'_HD_4',STAGE+'_HD_5',STAGE+'_angle_PCA_y_1',STAGE+'_angle_PCA_y_2',STAGE+'_angle_PCA_y_3',STAGE+'_angle_PCA_y_4',STAGE+'_angle_PCA_y_5',STAGE+'_angle_PCA_yhat_1',STAGE+'_angle_PCA_yhat_2',STAGE+'_angle_PCA_yhat_3',STAGE+'_angle_PCA_yhat_4',STAGE+'_angle_PCA_yhat_5',STAGE+'_angle_inertia_y_1',STAGE+'_angle_inertia_y_2',STAGE+'_angle_inertia_y_3',STAGE+'_angle_inertia_y_4',STAGE+'_angle_inertia_y_5',STAGE+'_angle_inertia_yhat_1',STAGE+'_angle_inertia_yhat_2',STAGE+'_angle_inertia_yhat_3',STAGE+'_angle_inertia_yhat_4',STAGE+'_angle_inertia_yhat_5',STAGE+'_angle_ellipse_y_1',STAGE+'_angle_ellipse_y_2',STAGE+'_angle_ellipse_y_3',STAGE+'_angle_ellipse_y_4',STAGE+'_angle_ellipse_y_5',STAGE+'_angle_ellipse_yhat_1',STAGE+'_angle_ellipse_yhat_2',STAGE+'_angle_ellipse_yhat_3',STAGE+'_angle_ellipse_yhat_4',STAGE+'_angle_ellipse_yhat_5'])\n",
    "df = df.T\n",
    "# df.to_csv('{}/metrics.csv'.format(STAGE),index=False)\n",
    "# df.to_csv('20220428_DrRyu_test.csv',index=False)\n",
    "# df.to_csv(f'20220428_{STAGE}_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(xs)):\n",
    "    print(idx, fnames[idx], DSCs[idx], HDs[idx])\n",
    "    visualize(\n",
    "        x=xs[idx][0], \n",
    "        y1=ys[idx][0][0],\n",
    "        y2=ys[idx][0][1],\n",
    "        y3=ys[idx][0][2],\n",
    "        y4=ys[idx][0][3],\n",
    "        y5=ys[idx][0][4], \n",
    "    )\n",
    "    visualize(\n",
    "        x=xs[idx][0], \n",
    "        yhat1 =yhats[idx][0][0].round(),\n",
    "        yhat2 =yhats[idx][0][1].round(),\n",
    "        yhat3 =yhats[idx][0][2].round(),\n",
    "        yhat4 =yhats[idx][0][3].round(),\n",
    "        yhat5 =yhats[idx][0][4].round(),\n",
    "    )\n",
    "\n",
    "#     visualize(\n",
    "#         x=xs[idx][0], \n",
    "#         yhat1_contour = mask2contour(yhats[idx][0][0].round()),\n",
    "#         yhat2_contour = mask2contour(yhats[idx][0][1].round()),\n",
    "#         yhat3_contour = mask2contour(yhats[idx][0][2].round()),\n",
    "#         yhat4_contour = mask2contour(yhats[idx][0][3].round()),\n",
    "#         yhat5_contour = mask2contour(yhats[idx][0][4].round()),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ACTIVE LEARNING\n",
    "# 1st DICE\n",
    "# class  1, DSC 0.978, HD 4.811\n",
    "# class  2, DSC 0.968, HD 3.895\n",
    "# class  3, DSC 0.921, HD 6.931\n",
    "# class  4, DSC 0.934, HD 6.170\n",
    "# class  5, DSC 0.919, HD 3.586\n",
    "\n",
    "# 2nd DICE\n",
    "# class  1, DSC 0.981, HD 4.662\n",
    "# class  2, DSC 0.976, HD 3.885\n",
    "# class  3, DSC 0.969, HD 4.518\n",
    "# class  4, DSC 0.954, HD 5.234\n",
    "# class  5, DSC 0.928, HD 3.575\n",
    "\n",
    "# 3rd DICE\n",
    "# class  1, DSC 0.986, HD 4.392\n",
    "# class  2, DSC 0.978, HD 3.703\n",
    "# class  3, DSC 0.975, HD 4.536\n",
    "# class  4, DSC 0.960, HD 5.145\n",
    "# class  5, DSC 0.932, HD 3.469\n",
    "\n",
    "# 4th DICE\n",
    "# class  1, DSC 0.986, HD 4.434\n",
    "# class  2, DSC 0.978, HD 3.783\n",
    "# class  3, DSC 0.976, HD 4.383\n",
    "# class  4, DSC 0.964, HD 5.135\n",
    "# class  5, DSC 0.931, HD 3.518\n",
    "\n",
    "# NONACTIVE LEARNING\n",
    "# 1st DICE\n",
    "# class  1, DSC 0.978, HD 4.811\n",
    "# class  2, DSC 0.968, HD 3.895\n",
    "# class  3, DSC 0.921, HD 6.931\n",
    "# class  4, DSC 0.934, HD 6.170\n",
    "# class  5, DSC 0.919, HD 3.586\n",
    "\n",
    "# 2nd DICE\n",
    "# class  1, DSC 0.985, HD 4.503\n",
    "# class  2, DSC 0.977, HD 3.919\n",
    "# class  3, DSC 0.974, HD 4.317\n",
    "# class  4, DSC 0.950, HD 5.492\n",
    "# class  5, DSC 0.932, HD 3.520\n",
    "\n",
    "# 3rd DICE\n",
    "# class  1, DSC 0.984, HD 4.536\n",
    "# class  2, DSC 0.976, HD 3.787\n",
    "# class  3, DSC 0.973, HD 4.421\n",
    "# class  4, DSC 0.955, HD 5.318\n",
    "# class  5, DSC 0.928, HD 3.630\n",
    "\n",
    "# 4th DICE\n",
    "# class  1, DSC 0.986, HD 4.434\n",
    "# class  2, DSC 0.978, HD 3.783\n",
    "# class  3, DSC 0.976, HD 4.383\n",
    "# class  4, DSC 0.964, HD 5.135\n",
    "# class  5, DSC 0.931, HD 3.518"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(xs)):\n",
    "    print(idx, fnames[idx], DSCs[idx], HDs[idx])\n",
    "    visualize(\n",
    "        x=xs[idx][0], \n",
    "        y1=ys[idx][0][0],\n",
    "        y2=ys[idx][0][1],\n",
    "        y3=ys[idx][0][2],\n",
    "        y4=ys[idx][0][3],\n",
    "        y5=ys[idx][0][4], \n",
    "    )\n",
    "    visualize(\n",
    "        x=xs[idx][0], \n",
    "        yhat1 =yhats[idx][0][0].round(),\n",
    "        yhat2 =yhats[idx][0][1].round(),\n",
    "        yhat3 =yhats[idx][0][2].round(),\n",
    "        yhat4 =yhats[idx][0][3].round(),\n",
    "        yhat5 =yhats[idx][0][4].round(),\n",
    "    )\n",
    "\n",
    "#     visualize(\n",
    "#         x=xs[idx][0], \n",
    "#         yhat1_contour = mask2contour(yhats[idx][0][0].round()),\n",
    "#         yhat2_contour = mask2contour(yhats[idx][0][1].round()),\n",
    "#         yhat3_contour = mask2contour(yhats[idx][0][2].round()),\n",
    "#         yhat4_contour = mask2contour(yhats[idx][0][3].round()),\n",
    "#         yhat5_contour = mask2contour(yhats[idx][0][4].round()),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import label, regionprops, regionprops_table\n",
    "import math\n",
    "\n",
    "def mask2contour(mask):\n",
    "    mask = mask.astype(np.uint8)\n",
    "    contour = cv2.Laplacian(mask,cv2.CV_8U,ksize=7)\n",
    "    result = skimage.morphology.dilation(contour,disk(4))\n",
    "    result = skimage.morphology.erosion(result,disk(4))\n",
    "    result[result!=0] = 1\n",
    "    return result\n",
    "\n",
    "# inertia axis\n",
    "def visualize(**images):\n",
    "    \"\"\"Plot images in one row.\"\"\"\n",
    "    angles = list()\n",
    "    \n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(32, 16))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        if i==0 and image.shape[0] == 3:\n",
    "            image_main = image.permute(1,2,0).numpy()#.int()\n",
    "        if torch.is_tensor(image) and image.shape[0] == 3:\n",
    "            image = image.permute(1,2,0).numpy()#.int()\n",
    "        else:\n",
    "            try:\n",
    "                image = image.numpy()\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image_main,cmap='gray')\n",
    "        if i>=1:\n",
    "            prop = inertia_axis(image)\n",
    "            coord_x = int(prop[0])\n",
    "            coord_y = int(prop[1])\n",
    "            angle = prop[2]\n",
    "            offset= 200\n",
    "            image[coord_y-5:coord_y+5,coord_x-5:coord_x+5] = 0\n",
    "            image = cv2.line(image,(coord_x-offset, coord_y-int(offset*np.tan(angle))),(coord_x+offset, coord_y+int(offset*np.tan(angle))),(0,255,255),5)\n",
    "            angle = np.degrees(angle)*(-1)\n",
    "            angles.append(angle)\n",
    "            plt.title(' '.join(name.split('_')).title())\n",
    "            plt.imshow(image,cmap='gray',alpha=0.6)\n",
    "            plt.tight_layout()\n",
    "#             plt.plot(coord_x, coord_y, '.g', markersize=15)\n",
    "    plt.show()\n",
    "    return angles\n",
    "\n",
    "for idx in range(len(xs)):\n",
    "    print(idx, fnames[idx], DSCs[idx], HDs[idx])\n",
    "    visualize(\n",
    "        x=xs[idx][0], \n",
    "        yhat1_contour = mask2contour(yhats[idx][0][0].round()),\n",
    "        yhat2_contour = mask2contour(yhats[idx][0][1].round()),\n",
    "        yhat3_contour = mask2contour(yhats[idx][0][2].round()),\n",
    "        yhat4_contour = mask2contour(yhats[idx][0][3].round()),\n",
    "        yhat5_contour = mask2contour(yhats[idx][0][4].round()),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = label2Contour(torch.tensor(np.expand_dims(yhats[idx][0][0].round(),0)))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import LabelToContour\n",
    "label2Contour = LabelToContour()\n",
    "for idx in range(len(xs)):\n",
    "    print(idx, fnames[idx], DSCs[idx], HDs[idx])\n",
    "    visualize(\n",
    "        x=xs[idx][0], \n",
    "        yhat1_contour = label2Contour(yhats[idx][0][0].round()),\n",
    "        yhat2_contour = label2Contour(yhats[idx][0][1].round()),\n",
    "        yhat3_contour = label2Contour(yhats[idx][0][2].round()),\n",
    "        yhat4_contour = label2Contour(yhats[idx][0][3].round()),\n",
    "        yhat5_contour = label2Contour(yhats[idx][0][4].round()),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import nibabel as nib\n",
    "import cv2, skimage\n",
    "from skimage.morphology import disk\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "metricfn = f1_score\n",
    "metricfn = directed_hausdorff\n",
    "\n",
    "def mask2contour(mask):\n",
    "    mask = mask.astype(np.uint8)\n",
    "    contour = cv2.Laplacian(mask,cv2.CV_8U,ksize=3)\n",
    "    result = skimage.morphology.dilation(contour,disk(4))\n",
    "    result = skimage.morphology.erosion(result,disk(4))\n",
    "    result[result!=0] = 1\n",
    "    return result\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    \n",
    "    xs = list()\n",
    "    ys = list()\n",
    "    yhats = list()\n",
    "    fnames = list()\n",
    "    DSCs = list()\n",
    "    HDs = list()\n",
    "    \n",
    "    for idx, batch in tqdm.tqdm(enumerate(etest_loader)):\n",
    "        x, y_seg,fname = batch['x'],batch['y_seg'], batch['fname']\n",
    "        \n",
    "        xs.append(x)\n",
    "        ys.append(y_seg)\n",
    "        fnames.append(fname)\n",
    "        \n",
    "        x, y_seg = x.cuda(), y_seg.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            yhat = model(x)            \n",
    "            yhats.append(yhat.cpu().detach().numpy())\n",
    "\n",
    "            temp_DSC = np.zeros((5))\n",
    "            temp_HD = np.zeros((5))\n",
    "            for idx_class in range(yhat.shape[1]):\n",
    "                DSC = f1_score(ys[idx][0][idx_class].flatten(), yhats[idx][0][idx_class].round().flatten())\n",
    "                HD = directed_hausdorff(ys[idx][0][idx_class], yhats[idx][0][idx_class].round())[0]\n",
    "                \n",
    "                temp_DSC[idx_class] = DSC\n",
    "                temp_HD[idx_class] = HD\n",
    "            DSCs.append(temp_DSC)\n",
    "            HDs.append(temp_HD)\n",
    "            \n",
    "    DSCs = np.array(DSCs)\n",
    "    HDs = np.array(HDs)\n",
    "    \n",
    "    return {'xs':xs,'ys':ys, 'yhats':yhats, 'fnames':fnames, 'DSCs':DSCs, 'HDs':HDs }\n",
    "\n",
    "result = test()\n",
    "\n",
    "xs= result['xs']\n",
    "ys = result['ys']\n",
    "yhats = result['yhats']\n",
    "fnames = result['fnames']\n",
    "DSCs = result['DSCs']\n",
    "HDs = result['HDs']\n",
    "# np.save('result_'+STAGE+'.npy', result)\n",
    "\n",
    "print(len(xs),len(ys),len(yhats),len(fnames),len(DSCs),len(HDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(xs)):\n",
    "    print(idx, fnames[idx], DSCs[idx], HDs[idx])\n",
    "    visualize(\n",
    "        x=xs[idx][0], \n",
    "        y1=ys[idx][0][0],\n",
    "        y2=ys[idx][0][1],\n",
    "        y3=ys[idx][0][2],\n",
    "        y4=ys[idx][0][3],\n",
    "        y5=ys[idx][0][4], \n",
    "    )\n",
    "    visualize(\n",
    "        x=xs[idx][0], \n",
    "        yhat1 =yhats[idx][0][0].round(),\n",
    "        yhat2 =yhats[idx][0][1].round(),\n",
    "        yhat3 =yhats[idx][0][2].round(),\n",
    "        yhat4 =yhats[idx][0][3].round(),\n",
    "        yhat5 =yhats[idx][0][4].round(),\n",
    "    )\n",
    "\n",
    "    visualize(\n",
    "        x=xs[idx][0], \n",
    "        yhat1_contour = mask2contour(yhats[idx][0][0].round()),\n",
    "        yhat2_contour = mask2contour(yhats[idx][0][1].round()),\n",
    "        yhat3_contour = mask2contour(yhats[idx][0][2].round()),\n",
    "        yhat4_contour = mask2contour(yhats[idx][0][3].round()),\n",
    "        yhat5_contour = mask2contour(yhats[idx][0][4].round()),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    \n",
    "    xs = list()\n",
    "    ys = list()\n",
    "    yhats = list()\n",
    "    fnames = list()    \n",
    "    DSCs = list()\n",
    "    HDs = list()\n",
    "\n",
    "    \n",
    "    for idx, batch in tqdm.tqdm(enumerate(inference_loader)):\n",
    "        x, y_seg,fname = batch['x'],batch['y_seg'], batch['fname']\n",
    "        \n",
    "        xs.append(x)\n",
    "        ys.append(y_seg)\n",
    "        fnames.append(fname)\n",
    "        \n",
    "        x, y_seg = x.cuda(), y_seg.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            yhat = model(x)            \n",
    "            yhats.append(yhat.cpu().detach().numpy())\n",
    "\n",
    "            temp_DSC = np.zeros((5))\n",
    "            temp_HD = np.zeros((5))\n",
    "            for idx_class in range(yhat.shape[1]):\n",
    "                DSC = f1_score(ys[idx][0][idx_class].flatten(), yhats[idx][0][idx_class].round().flatten())\n",
    "                HD = directed_hausdorff(ys[idx][0][idx_class], yhats[idx][0][idx_class].round())[0]\n",
    "                \n",
    "                temp_DSC[idx_class] = DSC\n",
    "                temp_HD[idx_class] = HD\n",
    "            DSCs.append(temp_DSC)\n",
    "            HDs.append(temp_HD)\n",
    "            \n",
    "    DSCs = np.array(DSCs)\n",
    "    HDs = np.array(HDs)\n",
    "    \n",
    "    return {'xs':xs,'ys':ys, 'yhats':yhats, 'fnames':fnames, 'DSCs':DSCs, 'HDs':HDs }\n",
    "\n",
    "result = test()\n",
    "\n",
    "xs= result['xs']\n",
    "ys = result['ys']\n",
    "yhats = result['yhats']\n",
    "fnames = result['fnames']\n",
    "DSCs = result['DSCs']\n",
    "HDs = result['HDs']\n",
    "\n",
    "print(len(xs),len(ys),len(yhats),len(fnames),len(DSCs),len(HDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_path_tosave = STAGE+'/yhat_inference/'\n",
    "for idx in trange(len(yhats)):\n",
    "    for idx_class in range(yhats[idx].shape[1]):\n",
    "        print(STAGE+'/DCM_inference/'+fnames[idx][0].split('/')[-1].split('.')[0]+'*')\n",
    "        dcm = glob.glob(STAGE+'/DCM_inference/'+fnames[idx][0].split('/')[-1].split('.')[0]+'*')[0]\n",
    "        dcm = pydicom.dcmread(dcm)\n",
    "        \n",
    "        def pad_augmentation():\n",
    "            return albu.Compose([albu.PadIfNeeded(dcm.Rows-700, dcm.Columns, border_mode=cv2.BORDER_CONSTANT, value=0, always_apply=True)])\n",
    "        \n",
    "        pad = pad_augmentation()\n",
    "        \n",
    "        affine = np.array([[dcm.PixelSpacing[0],0,0,0],[0,dcm.PixelSpacing[1],0,0],[0,0,1,0],[0,0,0,1]])\n",
    "        yhat_contour = mask2contour(yhats[idx][0][idx_class].round())\n",
    "        blank_upper = np.zeros((700,dcm.Columns))\n",
    "        target_height = dcm.Rows - 700\n",
    "        target_width = dcm.Columns\n",
    "#         yhat_contour = cv2.resize(yhat_contour,(2048,target_height),cv2.INTER_CUBIC)\n",
    "        yhat_contour = cv2.resize(yhat_contour,(2048,1024),cv2.INTER_CUBIC)\n",
    "        sample = pad(image=yhat_contour, mask=yhat_contour)\n",
    "        _, yhat_contour = sample['image'], sample['mask']\n",
    "        print(blank_upper.shape,yhat_contour.shape)\n",
    "\n",
    "        result = np.vstack([blank_upper,yhat_contour],)\n",
    "        result = np.transpose(result)\n",
    "        print(idx,fnames[idx],result.shape)\n",
    "        result = nib.Nifti1Image(result.astype(np.uint16), affine)\n",
    "        \n",
    "        os.makedirs(y_path_tosave, mode=0o777, exist_ok=True)\n",
    "        nib.save(result,STAGE+'/yhat_inference/'+fnames[idx][0].split('/')[-1].split('.')[0]+'_'+str(idx_class+1)+'.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_width,target_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(xs)):\n",
    "    print(idx, fnames[idx], DSCs[idx], HDs[idx])\n",
    "    visualize(\n",
    "        x=xs[idx][0], \n",
    "        y1=ys[idx][0][0],\n",
    "        y2=ys[idx][0][1],\n",
    "        y3=ys[idx][0][2],\n",
    "        y4=ys[idx][0][3],\n",
    "        y5=ys[idx][0][4], \n",
    "    )\n",
    "    visualize(\n",
    "        x=xs[idx][0], \n",
    "        yhat1 =yhats[idx][0][0].round(),\n",
    "        yhat2 =yhats[idx][0][1].round(),\n",
    "        yhat3 =yhats[idx][0][2].round(),\n",
    "        yhat4 =yhats[idx][0][3].round(),\n",
    "        yhat5 =yhats[idx][0][4].round(),\n",
    "    )\n",
    "\n",
    "    visualize(\n",
    "        x=xs[idx][0], \n",
    "        yhat1_contour = mask2contour(yhats[idx][0][0].round()),\n",
    "        yhat2_contour = mask2contour(yhats[idx][0][1].round()),\n",
    "        yhat3_contour = mask2contour(yhats[idx][0][2].round()),\n",
    "        yhat4_contour = mask2contour(yhats[idx][0][3].round()),\n",
    "        yhat5_contour = mask2contour(yhats[idx][0][4].round()),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import natsort\n",
    "import glob\n",
    "import pydicom\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "\n",
    "dcms = natsort.natsorted(glob.glob('4th/DCM_test/*.dcm'))\n",
    "labels = natsort.natsorted(glob.glob('4th/LABEL_test/*_5.nii.gz'))\n",
    "len(dcms),len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dcm.flatten(),bins=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without\n",
    "plt.hist(dcm.flatten(), bins=1024, cumulative=True, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with clipping\n",
    "plt.hist(dcm.flatten(), bins=1024, cumulative=True, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.morphology import binary_dilation, disk\n",
    "for idx in range(1):\n",
    "    dcm = pydicom.dcmread(dcms[idx])\n",
    "    dcm = dcm.pixel_array\n",
    "    dcm = dcm[300:]\n",
    "    label = nib.load(labels[idx])\n",
    "    label = label.get_fdata()\n",
    "    label = label[...,0]\n",
    "    label = np.moveaxis(label,0,1)\n",
    "    label = label[300:]\n",
    "    plt.figure(figsize=(20,12))\n",
    "    plt.imshow(dcm,cmap='gray')\n",
    "#     plt.imshow(binary_dilation(label,disk(4)),alpha=0.3)#,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    fname = dcms[idx].split('/')[-1][:-4]+'_5.png'\n",
    "#     print(fname)\n",
    "#     plt.savefig(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('scoliosis_downtask_result.csv')\n",
    "\n",
    "from sklearn.metrics import *\n",
    "def metrics(yhat,y):\n",
    "    \"\"\"\n",
    "    Binary classification metric\n",
    "    \n",
    "    input : long type inputs torch or numpy\n",
    "    output : various metric in dictionary form\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        try:\n",
    "            yhat = yhat.flatten().cpu().detach().numpy()\n",
    "            y = y.flatten().cpu().detach().numpy()\n",
    "        except:\n",
    "            yhat = yhat.flatten().numpy()\n",
    "            y = y.flatten().numpy()\n",
    "    except:\n",
    "        yhat = yhat.flatten()\n",
    "        y = y.flatten()\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y, yhat).ravel()\n",
    "    accuracy = (tp+tn)/(tn+fp+fn+tp)\n",
    "    iou = tp/(tp+fp+fn)\n",
    "    dice = 2*tp/(2*tp+fp+fn)\n",
    "    specificity = tn / (tn+fp)\n",
    "    sensitivity = tp / (tp+fn)\n",
    "    ppv = tp / (tp+fp)\n",
    "    npv = tn / (tn+fn)\n",
    "    \n",
    "#     df = pd.DataFrame([accuracy,dice,iou,npv,sensitivity,specificity,ppv],['accuracy','dice','iou','npv','sensitivity','specificity','ppv'])\n",
    "#     return df\n",
    "    return {'accuracy':accuracy,\n",
    "            'dice':dice, \n",
    "            'iou':iou, \n",
    "            'npv':npv,\n",
    "            'sensitivity':sensitivity,\n",
    "            'specificity':specificity,\n",
    "            'ppv':ppv,\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "for threshold in thresholds: \n",
    "    y = df['y'].to_numpy()\n",
    "    yhat = df['yhat'].to_numpy()\n",
    "\n",
    "    yhat_ = yhat.copy()\n",
    "    yhat_[yhat<threshold] = 0\n",
    "    yhat_[yhat>=threshold] = 1\n",
    "    print(threshold,metrics(yhat_,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
